{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"TensorX is a high-level deep neural network library written in Python that simplifies model specification, training, and execution using TensorFlow . It was designed for fast prototyping with minimum verbose and provides a set of modular components with a user-centric consistent API. Design Philosophy TensorX aims to be simple but sophisticated without a code base plagued by unnecessary abstractions and over-engineering and without sacrificing performance . It uses Tensorflow without hiding it completely behind a new namespace, it's mean to be a complement instead of a complete abstraction. The design mixes functional dataflow computation graphs with object-oriented neural network layer building blocks that are easy to add to and extend . Feature Summary Neural Network layer building blocks like Input , Linear , Lookup ; New TensorFlow ops : gumbel_top , logit , sinkhorn , etc; Graph Utils : allow for validation and compilation of layer graphs; Model Class : for easy inference , training , and evaluation ; Training Loop : easily customizable with a Callback system; Installation TensorX is written in pure python but depends on Tensorflow , which needs to be installed from the tensorflow package. The reason for this is that you might want to install Tensorflow builds optimized for your machine (see these ). Additionally, TensorX has optional dependencies like matplotlib or pygraphviz for certain functionality. Pip installation Install using pip with the following commands: pip install tensorflow pip install tensorx For more details about the installation, check the documentation . Test your installation import tensorflow as tf import tensorx as tx Documentation For details about TensorX API, tutorials, and other documentation, see https://tensorx.org . You can help by trying the project out, reporting bugs, suggest features, and by letting me know what you think. If you want to help, please read the contribution guide . Author Davide Nunes : get in touch @davidelnunes or by e-mail License Apache License 2.0","title":"Home"},{"location":"#_1","text":"","title":""},{"location":"#_2","text":"TensorX is a high-level deep neural network library written in Python that simplifies model specification, training, and execution using TensorFlow . It was designed for fast prototyping with minimum verbose and provides a set of modular components with a user-centric consistent API.","title":""},{"location":"#design-philosophy","text":"TensorX aims to be simple but sophisticated without a code base plagued by unnecessary abstractions and over-engineering and without sacrificing performance . It uses Tensorflow without hiding it completely behind a new namespace, it's mean to be a complement instead of a complete abstraction. The design mixes functional dataflow computation graphs with object-oriented neural network layer building blocks that are easy to add to and extend .","title":"Design Philosophy"},{"location":"#feature-summary","text":"Neural Network layer building blocks like Input , Linear , Lookup ; New TensorFlow ops : gumbel_top , logit , sinkhorn , etc; Graph Utils : allow for validation and compilation of layer graphs; Model Class : for easy inference , training , and evaluation ; Training Loop : easily customizable with a Callback system;","title":"Feature Summary"},{"location":"#installation","text":"TensorX is written in pure python but depends on Tensorflow , which needs to be installed from the tensorflow package. The reason for this is that you might want to install Tensorflow builds optimized for your machine (see these ). Additionally, TensorX has optional dependencies like matplotlib or pygraphviz for certain functionality.","title":"Installation"},{"location":"#pip-installation","text":"Install using pip with the following commands: pip install tensorflow pip install tensorx For more details about the installation, check the documentation .","title":"Pip installation"},{"location":"#test-your-installation","text":"import tensorflow as tf import tensorx as tx","title":"Test your installation"},{"location":"#documentation","text":"For details about TensorX API, tutorials, and other documentation, see https://tensorx.org . You can help by trying the project out, reporting bugs, suggest features, and by letting me know what you think. If you want to help, please read the contribution guide .","title":"Documentation"},{"location":"#author","text":"Davide Nunes : get in touch @davidelnunes or by e-mail","title":"Author"},{"location":"#license","text":"Apache License 2.0","title":"License"},{"location":"contributing/","text":"Contributing to TensorX First off, thanks for taking the time to contribute! These are mostly contribution guidelines, not rules. Feel free to propose changes to this document in a pull request. Community If you want to stay up to date with what is going on in the project, want to discuss features and use cases, talk about fixes or simply ask a question you can join the following channels: Users mailing list: https://groups.google.com/g/tensorx-users Discord Server: https://discord.gg/ZNeeQQHa Reporting bugs Bugs are tracked on the official issue tracker . When you are creating a bug report, please include as many details as possible. Use a clear and descriptive title for the issue to identify the problem; Make sure you have searched the issues to make sure the bug is not a duplicate; If you find a Closed issue that matches the problem you are experiencing, open a new issue and include a link to the original issue; Describe the exact steps which reproduce the problem in as many details as possible; Include details about your configuration and environment; Provide specific examples to demonstrate the steps to reproduce the issue; Describe the behavior you observed after following the steps and point out what exactly is the problem with that behavior; Explain which behavior you expected to see instead and why. Suggesting enhancements Suggestions are also tracked on the official issue tracker . Make sure you have searched the issues to make sure your suggestion is not a duplicate. Use a clear and descriptive title for the issue to identify the suggestion. Provide a step-by-step description of the suggested enhancement in as many details as possible. Provide specific use cases for your suggestion. Describe the current behavior and explain which behavior you expected to see instead and why. Contributing to code I recommend using Poetry to setup a local environment where you can contribute to and test TensorX codebase. You will first need to clone the repository using git and place yourself in its directory: git clone git@github.com:davidenunes/tensorx.git cd tensorx With Poetry installed, you will need to create a new virtualenv where the required dependencies for TensorX will be installed. After that, be sure that the current tests are passing on your machine: poetry install poetry run pytest tests/ Pull Requests Include unit tests when you change code or contribute new features, as they help to a) prove that your code works correctly, and b) guard against future breaking changes to lower the maintenance cost. Bug fixes require unit tests. The presence of a bug usually indicates insufficient test coverage. Add/Update documentation for the contributed code. docstrings use the Google style .","title":"Contributing"},{"location":"contributing/#contributing-to-tensorx","text":"First off, thanks for taking the time to contribute! These are mostly contribution guidelines, not rules. Feel free to propose changes to this document in a pull request.","title":"Contributing to TensorX"},{"location":"contributing/#community","text":"If you want to stay up to date with what is going on in the project, want to discuss features and use cases, talk about fixes or simply ask a question you can join the following channels: Users mailing list: https://groups.google.com/g/tensorx-users Discord Server: https://discord.gg/ZNeeQQHa","title":"Community"},{"location":"contributing/#reporting-bugs","text":"Bugs are tracked on the official issue tracker . When you are creating a bug report, please include as many details as possible. Use a clear and descriptive title for the issue to identify the problem; Make sure you have searched the issues to make sure the bug is not a duplicate; If you find a Closed issue that matches the problem you are experiencing, open a new issue and include a link to the original issue; Describe the exact steps which reproduce the problem in as many details as possible; Include details about your configuration and environment; Provide specific examples to demonstrate the steps to reproduce the issue; Describe the behavior you observed after following the steps and point out what exactly is the problem with that behavior; Explain which behavior you expected to see instead and why.","title":"Reporting bugs"},{"location":"contributing/#suggesting-enhancements","text":"Suggestions are also tracked on the official issue tracker . Make sure you have searched the issues to make sure your suggestion is not a duplicate. Use a clear and descriptive title for the issue to identify the suggestion. Provide a step-by-step description of the suggested enhancement in as many details as possible. Provide specific use cases for your suggestion. Describe the current behavior and explain which behavior you expected to see instead and why.","title":"Suggesting enhancements"},{"location":"contributing/#contributing-to-code","text":"I recommend using Poetry to setup a local environment where you can contribute to and test TensorX codebase. You will first need to clone the repository using git and place yourself in its directory: git clone git@github.com:davidenunes/tensorx.git cd tensorx With Poetry installed, you will need to create a new virtualenv where the required dependencies for TensorX will be installed. After that, be sure that the current tests are passing on your machine: poetry install poetry run pytest tests/","title":"Contributing to code"},{"location":"contributing/#pull-requests","text":"Include unit tests when you change code or contribute new features, as they help to a) prove that your code works correctly, and b) guard against future breaking changes to lower the maintenance cost. Bug fixes require unit tests. The presence of a bug usually indicates insufficient test coverage. Add/Update documentation for the contributed code. docstrings use the Google style .","title":"Pull Requests"},{"location":"api/","text":"","title":"Index"},{"location":"api/activation/","text":"tx.activation Activation functions are mainly used with the Activation layer, but these need not be TensorX functions only. Any function from Tensorflow or any generic function that takes tensors as inputs and outputs a Tensor or SparseTensor objects, can be used. This namespace is included for convenience and future extra activation functions. identity source . identity ( x , name : str = None ) Identity function Returns a tensor with the same content as the input tensor. Args x ( Tensor ) : The input tensor. name ( str ) : name for this op Returns tensor ( Tensor ) : of the same shape, type and content of the input tensor. sigmoid source . sigmoid ( x ) Sigmoid function Element-wise sigmoid function, defined as: f(x) = \\frac{1}{1 + \\exp(-x)} f(x) = \\frac{1}{1 + \\exp(-x)} Args x ( Tensor ) : A tensor or variable. Returns tensor ( Tensor ) : with the result of applying the sigmoid function to the input tensor. tanh source . tanh ( x ) Hyperbolic tangent (tanh) function. The element-wise hyperbolic tangent function is essentially a rescaled sigmoid function. The sigmoid function with range [0,1] [0,1] is defined as follows: f(x) = \\frac{1}{1 + \\exp(-x)} f(x) = \\frac{1}{1 + \\exp(-x)} the hyperbolic tangent is a re-scaled function such that it's outputs range [-1,1] [-1,1] defined as: $$ tanh(x) = 2f(2x)\u22121 $$ which leads us to the standard definition of hyperbolic tangent tanh(x)=\\frac{e^{x}-e^{-x}}{e^{x}+e^{-x}} tanh(x)=\\frac{e^{x}-e^{-x}}{e^{x}+e^{-x}} Args x ( Tensor ) : an input tensor Returns tensor ( Tensor ) : a tensor with the result of applying the element-wise hyperbolic tangent to the input relu source . relu ( x ) relu activation A Rectifier linear unit [1] is defined as: f(x)= \\max(0, x) f(x)= \\max(0, x) References (Vinod & Hinton, 2010) Rectified linear units improve restricted boltzmann machines Args x ( Tensor ) : input tensor Returns tensor ( Tensor ) that results in element-wise rectifier applied to x. elu source . elu ( x , alpha = 1.0 ) elu activation An Exponential Linear Unit (ELU) is defined as: f(x)=\\left\\{\\begin{array}{cc}x & x>0 \\\\ \\alpha \\cdot \\left(e^{x}-1\\right) & x<=0 \\end{array}\\right\\} f(x)=\\left\\{\\begin{array}{cc}x & x>0 \\\\ \\alpha \\cdot \\left(e^{x}-1\\right) & x<=0 \\end{array}\\right\\} References (Clevert et al. 2015) Fast and accurate deep network learning by exponential linear units (ELUs) . Args x ( Tensor ) : an input tensor alpha ( float ) : A scalar, slope of positive section. Returns tensor ( Tensor ) : resulting from the application of the elu activation to the input tensor. gelu source . gelu ( x , approximate : bool = True ) Gaussian Error Linear Unit. Computes gaussian error linear: 0.5 * x * (1 + tanh(sqrt(2 / pi) * (x + 0.044715 * x^3))) or x * P(X <= x) = 0.5 * x * (1 + erf(x / sqrt(2))) , where P(X) ~ N(0, 1), depending on whether approximation is enabled. References Gaussian Error Linear Units (GELUs) BERT . Args x ( Tensor ) : Must be one of the following types: float16 , float32 , float64 . approximate (bool) : whether to enable approximation. Returns tensor ( Tensor ) : with the same type as x softmax source . softmax ( x , axis = None , name = None ) softmax activation Softmax activation function, is equivalent to softmax = tf.exp(logits) / tf.reduce_sum(tf.exp(logits), axis) and it is defined as: \\sigma(\\mathbf{z})_{i}=\\frac{e^{z_{i}}}{\\sum_{j=1}^{K} e^{z_{j}}} \\sigma(\\mathbf{z})_{i}=\\frac{e^{z_{i}}}{\\sum_{j=1}^{K} e^{z_{j}}} Args x ( Tensor ) : input tensor axis ( int ) : the dimension softmax would be performed on. The default is -1 which indicates the last dimension. name ( str ) : name for this op Returns tensor ( Tensor ) : output resulting from the application of the softmax function to the input tensor sparsemax source . sparsemax ( logits , name : str = None ) Computes the sparsemax activation function [1] For each batch i and class j we have sparsemax[i, j] = max(logits[i, j] - tau(logits[i, :]), 0) References https://arxiv.org/abs/1602.02068 Args logits ( Tensor ) : tensor with dtype: half , float32 , float64 . name ( str ) : A name for the operation (optional). Returns tensor ( Tensor ) : with the same type as the input logits.","title":"activation"},{"location":"api/activation/#_1","text":"","title":""},{"location":"api/activation/#txactivation","text":"Activation functions are mainly used with the Activation layer, but these need not be TensorX functions only. Any function from Tensorflow or any generic function that takes tensors as inputs and outputs a Tensor or SparseTensor objects, can be used. This namespace is included for convenience and future extra activation functions.","title":"tx.activation"},{"location":"api/activation/#identity","text":"source . identity ( x , name : str = None ) Identity function Returns a tensor with the same content as the input tensor. Args x ( Tensor ) : The input tensor. name ( str ) : name for this op Returns tensor ( Tensor ) : of the same shape, type and content of the input tensor.","title":"identity"},{"location":"api/activation/#sigmoid","text":"source . sigmoid ( x ) Sigmoid function Element-wise sigmoid function, defined as: f(x) = \\frac{1}{1 + \\exp(-x)} f(x) = \\frac{1}{1 + \\exp(-x)} Args x ( Tensor ) : A tensor or variable. Returns tensor ( Tensor ) : with the result of applying the sigmoid function to the input tensor.","title":"sigmoid"},{"location":"api/activation/#tanh","text":"source . tanh ( x ) Hyperbolic tangent (tanh) function. The element-wise hyperbolic tangent function is essentially a rescaled sigmoid function. The sigmoid function with range [0,1] [0,1] is defined as follows: f(x) = \\frac{1}{1 + \\exp(-x)} f(x) = \\frac{1}{1 + \\exp(-x)} the hyperbolic tangent is a re-scaled function such that it's outputs range [-1,1] [-1,1] defined as: $$ tanh(x) = 2f(2x)\u22121 $$ which leads us to the standard definition of hyperbolic tangent tanh(x)=\\frac{e^{x}-e^{-x}}{e^{x}+e^{-x}} tanh(x)=\\frac{e^{x}-e^{-x}}{e^{x}+e^{-x}} Args x ( Tensor ) : an input tensor Returns tensor ( Tensor ) : a tensor with the result of applying the element-wise hyperbolic tangent to the input","title":"tanh"},{"location":"api/activation/#relu","text":"source . relu ( x ) relu activation A Rectifier linear unit [1] is defined as: f(x)= \\max(0, x) f(x)= \\max(0, x) References (Vinod & Hinton, 2010) Rectified linear units improve restricted boltzmann machines Args x ( Tensor ) : input tensor Returns tensor ( Tensor ) that results in element-wise rectifier applied to x.","title":"relu"},{"location":"api/activation/#elu","text":"source . elu ( x , alpha = 1.0 ) elu activation An Exponential Linear Unit (ELU) is defined as: f(x)=\\left\\{\\begin{array}{cc}x & x>0 \\\\ \\alpha \\cdot \\left(e^{x}-1\\right) & x<=0 \\end{array}\\right\\} f(x)=\\left\\{\\begin{array}{cc}x & x>0 \\\\ \\alpha \\cdot \\left(e^{x}-1\\right) & x<=0 \\end{array}\\right\\} References (Clevert et al. 2015) Fast and accurate deep network learning by exponential linear units (ELUs) . Args x ( Tensor ) : an input tensor alpha ( float ) : A scalar, slope of positive section. Returns tensor ( Tensor ) : resulting from the application of the elu activation to the input tensor.","title":"elu"},{"location":"api/activation/#gelu","text":"source . gelu ( x , approximate : bool = True ) Gaussian Error Linear Unit. Computes gaussian error linear: 0.5 * x * (1 + tanh(sqrt(2 / pi) * (x + 0.044715 * x^3))) or x * P(X <= x) = 0.5 * x * (1 + erf(x / sqrt(2))) , where P(X) ~ N(0, 1), depending on whether approximation is enabled. References Gaussian Error Linear Units (GELUs) BERT . Args x ( Tensor ) : Must be one of the following types: float16 , float32 , float64 . approximate (bool) : whether to enable approximation. Returns tensor ( Tensor ) : with the same type as x","title":"gelu"},{"location":"api/activation/#softmax","text":"source . softmax ( x , axis = None , name = None ) softmax activation Softmax activation function, is equivalent to softmax = tf.exp(logits) / tf.reduce_sum(tf.exp(logits), axis) and it is defined as: \\sigma(\\mathbf{z})_{i}=\\frac{e^{z_{i}}}{\\sum_{j=1}^{K} e^{z_{j}}} \\sigma(\\mathbf{z})_{i}=\\frac{e^{z_{i}}}{\\sum_{j=1}^{K} e^{z_{j}}} Args x ( Tensor ) : input tensor axis ( int ) : the dimension softmax would be performed on. The default is -1 which indicates the last dimension. name ( str ) : name for this op Returns tensor ( Tensor ) : output resulting from the application of the softmax function to the input tensor","title":"softmax"},{"location":"api/activation/#sparsemax","text":"source . sparsemax ( logits , name : str = None ) Computes the sparsemax activation function [1] For each batch i and class j we have sparsemax[i, j] = max(logits[i, j] - tau(logits[i, :]), 0) References https://arxiv.org/abs/1602.02068 Args logits ( Tensor ) : tensor with dtype: half , float32 , float64 . name ( str ) : A name for the operation (optional). Returns tensor ( Tensor ) : with the same type as the input logits.","title":"sparsemax"},{"location":"api/init/","text":"tx.init Initializers allow you to pre-specify an initialization strategy, encoded in a Callable object, without knowing the shape and dtype of the Variable being initialized. Info Despite most initializers being accessible through Tensorflow , we decided to gather them here for the sake of namespace consistency (not all TF initializers are in tf.initializers) zeros_init source . zeros_init () Zeroes Initializer Initializer that generates tensors initialized to 0. Returns initializer (Callable) : an initializer that returns a tensor filled with 0 when called on a given shape. ones_init source . ones_init () Ones Initializer Initializer that generates tensors initialized to 1. Returns initializer (Callable) : an initializer that returns a tensor filled with 1 when called on a given shape. constant_init source . constant_init ( value = 0 ) Constant Initializer The resulting tensor is populated with values of type dtype, as specified by arguments value following the desired shape. The argument value can be a constant value, or a list of values of type dtype. If value is a list, then the length of the list must be less than or equal to the number of elements implied by the desired shape of the tensor. In the case where the total number of elements in value is less than the number of elements required by the tensor shape, the last element in value will be used to fill the remaining entries. If the total number of elements in value is greater than the number of elements required by the tensor shape, the initializer will raise a ValueError. Args value : A Python scalar, list or tuple of values, or a N-dimensional numpy array. All elements of the initialized variable will be set to the corresponding value in the value argument. Returns initializer (Callable) : an initializer that returns a tensor from the given specification and a given shape uniform_init source . uniform_init ( minval : float = - 0.05 , maxval : float = 0.05 , seed = None ) Random Uniform Initializer Initializer that generates tensors with a uniform distribution. Args minval : Lower bound of the range of random values to generate. maxval : Upper bound of the range of random values to generate. Defaults to 1 for float types. seed (int32/int64) : seed for random number generator Returns initializer (Callable) : an initializer that returns a tensor from the given specification and a given shape normal_init source . normal_init ( mean : float = 0.0 , stddev = 0.05 , seed = None ) Random Normal Initializer Initializer that generates tensors with a normal distribution. Args mean : Mean of the random values to generate. stddev : Standard deviation of the random values to generate. seed (int32/int64) : seed for random number generator Returns initializer (Callable) : an initializer that returns a tensor from the given specification and a given shape glorot_uniform_init source . glorot_uniform_init ( seed : Optional = None ) Glorot Uniform Initializer This initialisation keeps the scale of the gradients roughly the same in all layers to mitigate vanishing and exploding gradients see [1]. References [1] (Glorot and Bengio 2010), \"Understanding the difficulty of training deep feedforward neural networks\". Args seed (int32/int64) : seed for random number generator Returns initializer (Callable) : callable that creates an initial value from a given shape orthogonal_init source . orthogonal_init ( gain : float = 1.0 , seed = None ) Orthogonal initializer If the shape of the tensor to initialize is two-dimensional, it is initialized with an orthogonal matrix obtained from the QR decomposition of a matrix of random numbers drawn from a normal distribution. If the matrix has fewer rows than columns then the output will have orthogonal rows. Otherwise, the output will have orthogonal columns. If the shape of the tensor to initialize is more than two-dimensional, a matrix of shape (shape[0] * ... * shape[n - 2], shape[n - 1]) is initialized, where n is the length of the shape vector. The matrix is subsequently reshaped to give a tensor of the desired shape. References Exact solutions to the nonlinear dynamics of learning in deep linear neural networks Args gain (float) : multiplicative factor to apply to the orthogonal matrix seed (int32/int64) : seed for random number generator Returns initializer (Callable) : callable that creates an orthogonal matrix from a given shape identity_init source . identity_init ( gain : float = 1.0 ) Identity Initializer creates an identity matrix for a 2D shape Args gain (float) : multiplicative factor to be applied to the identity matrix Returns initializer (Callable) : callable that creates an identity matrix from a given 2D shape he_uniform_init source . he_uniform_init ( seed = None ) He Uniform Initializer also known as MSRA initialization It draws samples from a uniform distribution within [-l, l] [-l, l] where l = \\sqrt{\\frac{6}{fan_{in}}} l = \\sqrt{\\frac{6}{fan_{in}}} where fan_{in} fan_{in} is the number of input units in the weight tensor. References Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification Args seed (int32/int64) : seed for random number generator Returns initializer (Callable) : callable that returns a tensor value from a given shape","title":"init"},{"location":"api/init/#_1","text":"","title":""},{"location":"api/init/#txinit","text":"Initializers allow you to pre-specify an initialization strategy, encoded in a Callable object, without knowing the shape and dtype of the Variable being initialized. Info Despite most initializers being accessible through Tensorflow , we decided to gather them here for the sake of namespace consistency (not all TF initializers are in tf.initializers)","title":"tx.init"},{"location":"api/init/#zeros_init","text":"source . zeros_init () Zeroes Initializer Initializer that generates tensors initialized to 0. Returns initializer (Callable) : an initializer that returns a tensor filled with 0 when called on a given shape.","title":"zeros_init"},{"location":"api/init/#ones_init","text":"source . ones_init () Ones Initializer Initializer that generates tensors initialized to 1. Returns initializer (Callable) : an initializer that returns a tensor filled with 1 when called on a given shape.","title":"ones_init"},{"location":"api/init/#constant_init","text":"source . constant_init ( value = 0 ) Constant Initializer The resulting tensor is populated with values of type dtype, as specified by arguments value following the desired shape. The argument value can be a constant value, or a list of values of type dtype. If value is a list, then the length of the list must be less than or equal to the number of elements implied by the desired shape of the tensor. In the case where the total number of elements in value is less than the number of elements required by the tensor shape, the last element in value will be used to fill the remaining entries. If the total number of elements in value is greater than the number of elements required by the tensor shape, the initializer will raise a ValueError. Args value : A Python scalar, list or tuple of values, or a N-dimensional numpy array. All elements of the initialized variable will be set to the corresponding value in the value argument. Returns initializer (Callable) : an initializer that returns a tensor from the given specification and a given shape","title":"constant_init"},{"location":"api/init/#uniform_init","text":"source . uniform_init ( minval : float = - 0.05 , maxval : float = 0.05 , seed = None ) Random Uniform Initializer Initializer that generates tensors with a uniform distribution. Args minval : Lower bound of the range of random values to generate. maxval : Upper bound of the range of random values to generate. Defaults to 1 for float types. seed (int32/int64) : seed for random number generator Returns initializer (Callable) : an initializer that returns a tensor from the given specification and a given shape","title":"uniform_init"},{"location":"api/init/#normal_init","text":"source . normal_init ( mean : float = 0.0 , stddev = 0.05 , seed = None ) Random Normal Initializer Initializer that generates tensors with a normal distribution. Args mean : Mean of the random values to generate. stddev : Standard deviation of the random values to generate. seed (int32/int64) : seed for random number generator Returns initializer (Callable) : an initializer that returns a tensor from the given specification and a given shape","title":"normal_init"},{"location":"api/init/#glorot_uniform_init","text":"source . glorot_uniform_init ( seed : Optional = None ) Glorot Uniform Initializer This initialisation keeps the scale of the gradients roughly the same in all layers to mitigate vanishing and exploding gradients see [1]. References [1] (Glorot and Bengio 2010), \"Understanding the difficulty of training deep feedforward neural networks\". Args seed (int32/int64) : seed for random number generator Returns initializer (Callable) : callable that creates an initial value from a given shape","title":"glorot_uniform_init"},{"location":"api/init/#orthogonal_init","text":"source . orthogonal_init ( gain : float = 1.0 , seed = None ) Orthogonal initializer If the shape of the tensor to initialize is two-dimensional, it is initialized with an orthogonal matrix obtained from the QR decomposition of a matrix of random numbers drawn from a normal distribution. If the matrix has fewer rows than columns then the output will have orthogonal rows. Otherwise, the output will have orthogonal columns. If the shape of the tensor to initialize is more than two-dimensional, a matrix of shape (shape[0] * ... * shape[n - 2], shape[n - 1]) is initialized, where n is the length of the shape vector. The matrix is subsequently reshaped to give a tensor of the desired shape. References Exact solutions to the nonlinear dynamics of learning in deep linear neural networks Args gain (float) : multiplicative factor to apply to the orthogonal matrix seed (int32/int64) : seed for random number generator Returns initializer (Callable) : callable that creates an orthogonal matrix from a given shape","title":"orthogonal_init"},{"location":"api/init/#identity_init","text":"source . identity_init ( gain : float = 1.0 ) Identity Initializer creates an identity matrix for a 2D shape Args gain (float) : multiplicative factor to be applied to the identity matrix Returns initializer (Callable) : callable that creates an identity matrix from a given 2D shape","title":"identity_init"},{"location":"api/init/#he_uniform_init","text":"source . he_uniform_init ( seed = None ) He Uniform Initializer also known as MSRA initialization It draws samples from a uniform distribution within [-l, l] [-l, l] where l = \\sqrt{\\frac{6}{fan_{in}}} l = \\sqrt{\\frac{6}{fan_{in}}} where fan_{in} fan_{in} is the number of input units in the weight tensor. References Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification Args seed (int32/int64) : seed for random number generator Returns initializer (Callable) : callable that returns a tensor value from a given shape","title":"he_uniform_init"},{"location":"api/loss/","text":"binary_cross_entropy source . binary_cross_entropy ( labels , logits , name = 'binary_cross_entropy' ) Binary Cross Entropy Measures the probability error in discrete binary classification tasks in which each class is independent and not mutually exclusive. On Entropy and Cross-Entropy Entropy refers to the number of bits required to transmit a randomly selected event from a probability distribution. A skewed distribution has a low entropy, whereas a distribution where events have equal probability has a larger entropy. The entropy of a random variable with a set x \\in X x \\in X discrete states and their probability P(x) P(x) , can be computed as: H(X) = \u2013\\sum_{x \\in X} P(x) * log(P(x)) H(X) = \u2013\\sum_{x \\in X} P(x) * log(P(x)) Cross-entropy builds upon this idea to compute the number of bits required to represent or transmit an average event from one distribution compared to another distribution. if we consider a target distribution P P and an approximation of the target distribution Q Q , the cross-entropy of Q Q from P P is the number of additional bits to represent an event using Q instead of P: H(P, Q) = \u2013\\sum_{x \\in X} P(x) * log(Q(x)) H(P, Q) = \u2013\\sum_{x \\in X} P(x) * log(Q(x)) Warning This is to be used on the logits of a model, not on the predicted labels. See also from TensorFlow . Args labels ( Tensor ) : empiric probability values (labels that occurred for a given sample) logits ( Tensor ) : unscaled log probabilities used to predict the labels with sigmoid(logits) name (str) : op name Returns tensor ( Tensor ) : binary (sigmoid) cross-entropy loss. categorical_cross_entropy source . categorical_cross_entropy ( labels , logits , axis = - 1 , name = 'categorical_cross_entropy' ) Categorical Cross entropy Measures the probability error in discrete classification tasks in which the classes are mutually exclusive. Warning This is to be used on the logits of a model, not on the predicted labels. Do not call this loss with the output of softmax. See also from TensorFlow . Args labels (Tensor) : empiric probability distribution. Each row labels[i] must be a valid probability distribution logits (Tensor) : unscaled log probabilities used to predict the labels with softmax(logits) axis (int) : The class dimension. Defaulted to -1 which is the last dimension. name (str) : op name (integrate to 1). Returns tensor ( Tensor ) : categorical (softmax) cross-entropy loss. mse source . mse ( target , predicted ) Mean Squared Error (MSE) Loss Measures the average of the squares of the errors - the difference between an estimator and what is estimated. This is a risk function, corresponding to the expected value of the quadratic loss: MSE =\\frac{1}{N}\u200b\\sum^{N}_{i=0}\u200b(y-\\hat{y})^2 MSE =\\frac{1}{N}\u200b\\sum^{N}_{i=0}\u200b(y-\\hat{y})^2 Info MSE is sensitive towards outliers and given several examples with the same input feature values, the optimal prediction will be their mean target value. This should be compared with Mean Absolute Error , where the optimal prediction is the median. MSE is thus good to use if you believe that your target data, conditioned on the input, is normally distributed around a mean value --and when it's important to penalize outliers. Args predicted ( Tensor ) : estimated target values target ( Tensor ) : ground truth, correct values Returns tensor ( Tensor ) : mean squared error value kld source . kld ( target , predicted ) Kullback\u2013Leibler Divergence Loss Kullback\u2013Leibler divergence (also called relative entropy) is a measure of how one probability distribution is different from a second, reference probability distribution. D_{KL}(P || Q) = - \\sum_{x \\in X}P(x) log\\left(\\frac{Q(x)}{P(x)}\\right) D_{KL}(P || Q) = - \\sum_{x \\in X}P(x) log\\left(\\frac{Q(x)}{P(x)}\\right) it is the expectation of the logarithmic difference between the probabilities P P and Q Q , where the expectation is taken using the probabilities P P . Args target ( Tensor ) : target probability distribution predicted ( Tensor ) : distribution predicted by the model Returns kld ( Tensor ) : LK divergence between the target and predicted distributions sinkhorn_loss source . sinkhorn_loss ( target , predicted , epsilon , n_iter , cost_fn = None ) Sinkhorn Loss Alias: * tx.metrics.sinkhorn Info Optimal Transport (OT) provides a framework from which one can define a more powerful geometry to compare probability distributions. This power comes, however, with a heavy computational price. The cost of computing OT distances scales at least in O(d^3 log(d)) O(d^3 log(d)) when comparing two histograms of dimension d d . Sinkhorn algorithm alleviate this problem by solving an regularized OT in linear time. Given two measures with n points each with locations x and y outputs an approximation of the Optimal Transport (OT) cost with regularization parameter epsilon, niter is the maximum number of steps in sinkhorn loop References Concerning nonnegative matrices and doubly stochastic matrices Sinkhorn Distances:Lightspeed Computation of Optimal Transport Args predicted ( Tensor ) : model distribution target ( Tensor ) : ground_truth, empirical distribution epsilon (float) : regularization term >0 n_iter (int) : number of sinkhorn iterations cost_fn (Callable) : function that returns the cost matrix between y_pred and y_true, defaults to |x_i-y_j|^p |x_i-y_j|^p . Returns cost ( Tensor ) : sinkhorn cost of moving from the mass from the model distribution y_pred to the empirical distribution y_true . sparsemax_loss source . sparsemax_loss ( logits , labels , name = 'sparsemax_loss' ) Sparsemax Loss A loss function for the sparsemax activation function. This is similar to tf.nn.softmax , but able to output s parse probabilities. Info Applicable to multi-label classification problems and attention-based neural networks (e.g. for natural language inference) References From Softmax to Sparsemax: A Sparse Model of Attention and Multi-Label Classification Args labels ( Tensor ) : the target dense labels (one hot encoded) logits ( Tensor ) : unnormalized log probabilities name (str) : op name Returns loss ( Tensor ) : sparsemax loss binary_hinge source . binary_hinge ( labels , logits ) Binary Hinge Loss Measures the classification error for maximum-margin classification. Margin classifiers like Support Vector Machines (SVM) maximise the distance between the closest examples and the decision boundary separating the binary classes. The hinge loss is defined as: \\ell(y) = \\max(0, 1-t \\cdot y), \\ell(y) = \\max(0, 1-t \\cdot y), where t t is the intended output (labels) and y y are the output logits from the classification decision function, not the predicted class label. Args labels ( Tensor ) : tensor with values -1 or 1. Binary (0 or 1) labels are converted to -1 or 1. logits ( Tensor ) : unscaled log probabilities. Returns tensor ( Tensor ) : hinge loss float tensor","title":"loss"},{"location":"api/loss/#_1","text":"","title":""},{"location":"api/loss/#binary_cross_entropy","text":"source . binary_cross_entropy ( labels , logits , name = 'binary_cross_entropy' ) Binary Cross Entropy Measures the probability error in discrete binary classification tasks in which each class is independent and not mutually exclusive. On Entropy and Cross-Entropy Entropy refers to the number of bits required to transmit a randomly selected event from a probability distribution. A skewed distribution has a low entropy, whereas a distribution where events have equal probability has a larger entropy. The entropy of a random variable with a set x \\in X x \\in X discrete states and their probability P(x) P(x) , can be computed as: H(X) = \u2013\\sum_{x \\in X} P(x) * log(P(x)) H(X) = \u2013\\sum_{x \\in X} P(x) * log(P(x)) Cross-entropy builds upon this idea to compute the number of bits required to represent or transmit an average event from one distribution compared to another distribution. if we consider a target distribution P P and an approximation of the target distribution Q Q , the cross-entropy of Q Q from P P is the number of additional bits to represent an event using Q instead of P: H(P, Q) = \u2013\\sum_{x \\in X} P(x) * log(Q(x)) H(P, Q) = \u2013\\sum_{x \\in X} P(x) * log(Q(x)) Warning This is to be used on the logits of a model, not on the predicted labels. See also from TensorFlow . Args labels ( Tensor ) : empiric probability values (labels that occurred for a given sample) logits ( Tensor ) : unscaled log probabilities used to predict the labels with sigmoid(logits) name (str) : op name Returns tensor ( Tensor ) : binary (sigmoid) cross-entropy loss.","title":"binary_cross_entropy"},{"location":"api/loss/#categorical_cross_entropy","text":"source . categorical_cross_entropy ( labels , logits , axis = - 1 , name = 'categorical_cross_entropy' ) Categorical Cross entropy Measures the probability error in discrete classification tasks in which the classes are mutually exclusive. Warning This is to be used on the logits of a model, not on the predicted labels. Do not call this loss with the output of softmax. See also from TensorFlow . Args labels (Tensor) : empiric probability distribution. Each row labels[i] must be a valid probability distribution logits (Tensor) : unscaled log probabilities used to predict the labels with softmax(logits) axis (int) : The class dimension. Defaulted to -1 which is the last dimension. name (str) : op name (integrate to 1). Returns tensor ( Tensor ) : categorical (softmax) cross-entropy loss.","title":"categorical_cross_entropy"},{"location":"api/loss/#mse","text":"source . mse ( target , predicted ) Mean Squared Error (MSE) Loss Measures the average of the squares of the errors - the difference between an estimator and what is estimated. This is a risk function, corresponding to the expected value of the quadratic loss: MSE =\\frac{1}{N}\u200b\\sum^{N}_{i=0}\u200b(y-\\hat{y})^2 MSE =\\frac{1}{N}\u200b\\sum^{N}_{i=0}\u200b(y-\\hat{y})^2 Info MSE is sensitive towards outliers and given several examples with the same input feature values, the optimal prediction will be their mean target value. This should be compared with Mean Absolute Error , where the optimal prediction is the median. MSE is thus good to use if you believe that your target data, conditioned on the input, is normally distributed around a mean value --and when it's important to penalize outliers. Args predicted ( Tensor ) : estimated target values target ( Tensor ) : ground truth, correct values Returns tensor ( Tensor ) : mean squared error value","title":"mse"},{"location":"api/loss/#kld","text":"source . kld ( target , predicted ) Kullback\u2013Leibler Divergence Loss Kullback\u2013Leibler divergence (also called relative entropy) is a measure of how one probability distribution is different from a second, reference probability distribution. D_{KL}(P || Q) = - \\sum_{x \\in X}P(x) log\\left(\\frac{Q(x)}{P(x)}\\right) D_{KL}(P || Q) = - \\sum_{x \\in X}P(x) log\\left(\\frac{Q(x)}{P(x)}\\right) it is the expectation of the logarithmic difference between the probabilities P P and Q Q , where the expectation is taken using the probabilities P P . Args target ( Tensor ) : target probability distribution predicted ( Tensor ) : distribution predicted by the model Returns kld ( Tensor ) : LK divergence between the target and predicted distributions","title":"kld"},{"location":"api/loss/#sinkhorn_loss","text":"source . sinkhorn_loss ( target , predicted , epsilon , n_iter , cost_fn = None ) Sinkhorn Loss Alias: * tx.metrics.sinkhorn Info Optimal Transport (OT) provides a framework from which one can define a more powerful geometry to compare probability distributions. This power comes, however, with a heavy computational price. The cost of computing OT distances scales at least in O(d^3 log(d)) O(d^3 log(d)) when comparing two histograms of dimension d d . Sinkhorn algorithm alleviate this problem by solving an regularized OT in linear time. Given two measures with n points each with locations x and y outputs an approximation of the Optimal Transport (OT) cost with regularization parameter epsilon, niter is the maximum number of steps in sinkhorn loop References Concerning nonnegative matrices and doubly stochastic matrices Sinkhorn Distances:Lightspeed Computation of Optimal Transport Args predicted ( Tensor ) : model distribution target ( Tensor ) : ground_truth, empirical distribution epsilon (float) : regularization term >0 n_iter (int) : number of sinkhorn iterations cost_fn (Callable) : function that returns the cost matrix between y_pred and y_true, defaults to |x_i-y_j|^p |x_i-y_j|^p . Returns cost ( Tensor ) : sinkhorn cost of moving from the mass from the model distribution y_pred to the empirical distribution y_true .","title":"sinkhorn_loss"},{"location":"api/loss/#sparsemax_loss","text":"source . sparsemax_loss ( logits , labels , name = 'sparsemax_loss' ) Sparsemax Loss A loss function for the sparsemax activation function. This is similar to tf.nn.softmax , but able to output s parse probabilities. Info Applicable to multi-label classification problems and attention-based neural networks (e.g. for natural language inference) References From Softmax to Sparsemax: A Sparse Model of Attention and Multi-Label Classification Args labels ( Tensor ) : the target dense labels (one hot encoded) logits ( Tensor ) : unnormalized log probabilities name (str) : op name Returns loss ( Tensor ) : sparsemax loss","title":"sparsemax_loss"},{"location":"api/loss/#binary_hinge","text":"source . binary_hinge ( labels , logits ) Binary Hinge Loss Measures the classification error for maximum-margin classification. Margin classifiers like Support Vector Machines (SVM) maximise the distance between the closest examples and the decision boundary separating the binary classes. The hinge loss is defined as: \\ell(y) = \\max(0, 1-t \\cdot y), \\ell(y) = \\max(0, 1-t \\cdot y), where t t is the intended output (labels) and y y are the output logits from the classification decision function, not the predicted class label. Args labels ( Tensor ) : tensor with values -1 or 1. Binary (0 or 1) labels are converted to -1 or 1. logits ( Tensor ) : unscaled log probabilities. Returns tensor ( Tensor ) : hinge loss float tensor","title":"binary_hinge"},{"location":"api/math/","text":"tx.math sparse_dense_multiply source . sparse_dense_multiply ( sp_tensor , dense_tensor , name = 'sparse_multiply_dense' ) element-wise sparse_multiply_dense Info Uses sparse_dense_cwise_mul from Tensorflow but returns a dense result and reshapes the result to match the shape of sp_tensor Args sp_tensor (SparseTensor) : a sparse tensor dense_tensor (Tensor) : a dense tensor name (str) : op name Returns tensor (Tensor) : the result for the multiplication between the sparse and dense tensors rms source . rms ( x ) Root mean square (RMS) Also known as quadratic mean is defined as: x_{\\mathrm{RMS}}=\\sqrt{\\frac{x_{1}^{2}+x_{2}^{2}+\\ldots+x_{n}^{2}}{n}} x_{\\mathrm{RMS}}=\\sqrt{\\frac{x_{1}^{2}+x_{2}^{2}+\\ldots+x_{n}^{2}}{n}} In estimation theory, the root-mean-square deviation of an estimator is a measure of the imperfection of the fit of the estimator to the data. Args x ( Tensor ) : input tensor Returns result ( Tensor ) : scalar tensor with the result of applying the root mean square to the input tensor","title":"math"},{"location":"api/math/#_1","text":"","title":""},{"location":"api/math/#txmath","text":"","title":"tx.math"},{"location":"api/math/#sparse_dense_multiply","text":"source . sparse_dense_multiply ( sp_tensor , dense_tensor , name = 'sparse_multiply_dense' ) element-wise sparse_multiply_dense Info Uses sparse_dense_cwise_mul from Tensorflow but returns a dense result and reshapes the result to match the shape of sp_tensor Args sp_tensor (SparseTensor) : a sparse tensor dense_tensor (Tensor) : a dense tensor name (str) : op name Returns tensor (Tensor) : the result for the multiplication between the sparse and dense tensors","title":"sparse_dense_multiply"},{"location":"api/math/#rms","text":"source . rms ( x ) Root mean square (RMS) Also known as quadratic mean is defined as: x_{\\mathrm{RMS}}=\\sqrt{\\frac{x_{1}^{2}+x_{2}^{2}+\\ldots+x_{n}^{2}}{n}} x_{\\mathrm{RMS}}=\\sqrt{\\frac{x_{1}^{2}+x_{2}^{2}+\\ldots+x_{n}^{2}}{n}} In estimation theory, the root-mean-square deviation of an estimator is a measure of the imperfection of the fit of the estimator to the data. Args x ( Tensor ) : input tensor Returns result ( Tensor ) : scalar tensor with the result of applying the root mean square to the input tensor","title":"rms"},{"location":"api/metrics/","text":"euclidean_distance source . euclidean_distance ( tensor1 , tensor2 ) Computes the euclidean distance between two tensors. The euclidean distance or L^2 L^2 distance between points p p and q q is the length of the line segment connecting them. distance(q,p) =\\sqrt{\\sum_{i=1}^{n}\\left(q_{i}-p_{i}\\right)^{2}} distance(q,p) =\\sqrt{\\sum_{i=1}^{n}\\left(q_{i}-p_{i}\\right)^{2}} Args tensor1 : a Tensor tensor2 : a Tensor dim : dimension along which the euclidean distance is computed Returns a Tensor with the euclidean distances between the two tensors cosine_distance source . cosine_distance ( tensor1 , tensor2 , dtype = tf . float32 ) cosine_distance Computes the pairwise cosine distance between two non-zero tensors on their last dimension. The cosine distance is defined as 1 - cosine similarity. With the cosine similarity defined as: similarity =\\cos (\\theta)=\\frac{\\mathbf{A} \\cdot \\mathbf{B}}{\\|\\mathbf{A}\\|\\|\\mathbf{B}\\|}=\\frac{ \\sum_{i=1}^{n} A_{i} B_{i}}{\\sqrt{\\sum_{i=1}^{n} A_{i}^{2}} \\sqrt{\\sum_{i=1}^{n} B_{i}^{2}}} similarity =\\cos (\\theta)=\\frac{\\mathbf{A} \\cdot \\mathbf{B}}{\\|\\mathbf{A}\\|\\|\\mathbf{B}\\|}=\\frac{ \\sum_{i=1}^{n} A_{i} B_{i}}{\\sqrt{\\sum_{i=1}^{n} A_{i}^{2}} \\sqrt{\\sum_{i=1}^{n} B_{i}^{2}}} Args tensor1 ( Tensor ) : first tensor tensor2 ( Tensor ) : second tensor dtype ( DType ) : assumed type of both tensors Returns distance ( Tensor ) : the pairwise cosine distance between two tensors torus_l1_distance source . torus_l1_distance ( point , shape ) Computes the l1 distance between a given point or batch of points and all other points in a torus Args point ( Tensor ) : a rank 0 or rank 1 tensor with the coordinates for a point or a rank 2 tensor with a batch of points. shape ( List ) : a list with the shape for the torus - either 1D or 2D Returns distances ( Tensor ) : a rank 1 or 2 tensor with the distances between each point in the 1D torus and each unique coordinate in the shape Examples distance for a single point torus_l1_distance(1,[4]) or torus_1d_l1_distance([1],[4]) [ 1. , 0. , 1. , 2. ] distance for multiple points torus_l1_distance([[2],[3]],[4]) [[ 2. , 1. , 0. , 1. ], [ 1. , 2. , 1. , 0. ]] distance between a point and other coordinates in a 2D torus r = torus_l1_distance ([[ 1 , 1 ],[ 1 , 2 ]],[ 3 , 3 ]) np . reshape ( r ,[ - 1 , 3 , 3 ]) [[[ 2. , 1. , 2. ], [ 1. , 0. , 1. ], [ 2. , 1. , 2. ]], [[ 2. , 2. , 1. ], [ 1. , 1. , 0. ], [ 2. , 2. , 1. ]]]","title":"metrics"},{"location":"api/metrics/#_1","text":"","title":""},{"location":"api/metrics/#euclidean_distance","text":"source . euclidean_distance ( tensor1 , tensor2 ) Computes the euclidean distance between two tensors. The euclidean distance or L^2 L^2 distance between points p p and q q is the length of the line segment connecting them. distance(q,p) =\\sqrt{\\sum_{i=1}^{n}\\left(q_{i}-p_{i}\\right)^{2}} distance(q,p) =\\sqrt{\\sum_{i=1}^{n}\\left(q_{i}-p_{i}\\right)^{2}} Args tensor1 : a Tensor tensor2 : a Tensor dim : dimension along which the euclidean distance is computed Returns a Tensor with the euclidean distances between the two tensors","title":"euclidean_distance"},{"location":"api/metrics/#cosine_distance","text":"source . cosine_distance ( tensor1 , tensor2 , dtype = tf . float32 ) cosine_distance Computes the pairwise cosine distance between two non-zero tensors on their last dimension. The cosine distance is defined as 1 - cosine similarity. With the cosine similarity defined as: similarity =\\cos (\\theta)=\\frac{\\mathbf{A} \\cdot \\mathbf{B}}{\\|\\mathbf{A}\\|\\|\\mathbf{B}\\|}=\\frac{ \\sum_{i=1}^{n} A_{i} B_{i}}{\\sqrt{\\sum_{i=1}^{n} A_{i}^{2}} \\sqrt{\\sum_{i=1}^{n} B_{i}^{2}}} similarity =\\cos (\\theta)=\\frac{\\mathbf{A} \\cdot \\mathbf{B}}{\\|\\mathbf{A}\\|\\|\\mathbf{B}\\|}=\\frac{ \\sum_{i=1}^{n} A_{i} B_{i}}{\\sqrt{\\sum_{i=1}^{n} A_{i}^{2}} \\sqrt{\\sum_{i=1}^{n} B_{i}^{2}}} Args tensor1 ( Tensor ) : first tensor tensor2 ( Tensor ) : second tensor dtype ( DType ) : assumed type of both tensors Returns distance ( Tensor ) : the pairwise cosine distance between two tensors","title":"cosine_distance"},{"location":"api/metrics/#torus_l1_distance","text":"source . torus_l1_distance ( point , shape ) Computes the l1 distance between a given point or batch of points and all other points in a torus Args point ( Tensor ) : a rank 0 or rank 1 tensor with the coordinates for a point or a rank 2 tensor with a batch of points. shape ( List ) : a list with the shape for the torus - either 1D or 2D Returns distances ( Tensor ) : a rank 1 or 2 tensor with the distances between each point in the 1D torus and each unique coordinate in the shape Examples distance for a single point torus_l1_distance(1,[4]) or torus_1d_l1_distance([1],[4]) [ 1. , 0. , 1. , 2. ] distance for multiple points torus_l1_distance([[2],[3]],[4]) [[ 2. , 1. , 0. , 1. ], [ 1. , 2. , 1. , 0. ]] distance between a point and other coordinates in a 2D torus r = torus_l1_distance ([[ 1 , 1 ],[ 1 , 2 ]],[ 3 , 3 ]) np . reshape ( r ,[ - 1 , 3 , 3 ]) [[[ 2. , 1. , 2. ], [ 1. , 0. , 1. ], [ 2. , 1. , 2. ]], [[ 2. , 2. , 1. ], [ 1. , 1. , 0. ], [ 2. , 2. , 1. ]]]","title":"torus_l1_distance"},{"location":"api/ops/","text":"tx.ops matrix_indices source . matrix_indices ( index_tensor , dtype = tf . int64 , sort_indices = True , name = 'matrix_indices' ) Transforms a batch of column indices into a batch of matrix indices Args index_tensor ( Tensor ) : a tensor with shape (b,n) with a batch of n column indices. dtype ( DType ) : the output dtype for the indices. Defaults to int64 . sort_indices ( bool ) : if True , output indices are sorted in canonical row-major order. name ( str ) : name for this op. Returns tensor ( Tensor ) : tensor with shape [b,2] for each index in the input tensor with the corresponding matrix indices empty_sparse_tensor source . empty_sparse_tensor ( dense_shape , dtype = tf . float32 , name = 'empty_sp_tensor' ) Creates an empty SparseTensor Args dense_shape ( TensorShape ) : a 1-D tensor, python list, or numpy array with the output shape for the sparse tensor dtype ( DType ) : the dtype of the values for the empty tf.SparseTensor name ( str ) : a name for this operation Returns sp_tensor ( SparseTensor ) : an empty sparse tensor with a given shape sparse_ones source . sparse_ones ( indices , dense_shape , dtype = tf . float32 , name = 'sparse_ones' ) Creates a new SparseTensor with the given indices having value 1 Args indices ( Tensor ) : a rank 2 tensor with the (row,column) indices for the resulting sparse tensor dense_shape ( Tensor or TensorShape ) : the output dense shape dtype ( tf.DType ) : the tensor type for the values name ( str ) : sparse_ones op Returns sp_tensor ( SparseTensor ) : a new sparse tensor with values set to 1 sparse_zeros source . sparse_zeros ( indices , dense_shape , dtype = tf . float32 , name = 'sparse_zeros' ) Creates a new SparseTensor with the given indices having value 0 Args indices ( Tensor ) : a rank 2 tensor with the (row,column) indices for the resulting sparse tensor dense_shape ( Tensor or TensorShape ) : the output dense shape dtype ( tf.DType ) : the tensor type for the values name ( str ) : sparse_ones op Returns sp_tensor ( SparseTensor ) : a new sparse tensor with values set to 0 sparse_overlap source . sparse_overlap ( sp_tensor1 , sp_tensor2 , name = 'sparse_overlap' ) sparse overlap Returns a SparseTensor where the indices of the overlapping indices in the two sparse tensors with the values of the first one. Args sp_tensor1 ( SparseTensor ) : a sparse tensor sp_tensor2 ( SparseTensor ) : another sparse tensor name ( str ) : name for sparse_overlap op Returns sp_tensor ( SparseTensor ) : sparse tensor with the overlapping indices and the values of sp_tensor1 apply_gate source . apply_gate ( tensor , gate ) Applies a gate tensor to the given input if input tensor outer dimension is a multiple of gate outer dimension we use broadcasting to apply the gate evenly across the input tensor. Example tx . apply_gate ( tf . ones ([ 1 , 4 ]),[ 1. , 0. ]) [[ 1. , 1. , 0. , 0. ]] Args tensor ( Tensor ) : an input tensor gate ( Tensor ) : float tensor that is multiplied by the input tensor. The outer dimension of the input tensor should either match the gate tensor or be a multiple of gate tensor. Returns gated ( Tensor ) : input tensor gated using the given gate weights sparse_indices source . sparse_indices ( sp_values , name = 'sparse_indices' ) Returns a SparseTensor with the values containing column indices for the active values on a given SparseTensor . Use Case To be used with embedding_lookup_sparse when we need two SparseTensor objects with the indices and values Args sp_values ( SparseTensor ) : a sparse tensor for which we extract the active indices. name ( str ) : name for sparse_indices op Returns sp_indices ( SparseTensor ) : a sparse tensor with the column indices sparse_matrix_indices source . sparse_matrix_indices ( column_indices , num_cols , dtype = tf . float32 , name = 'sparse_one_hot' ) Transforms a batch of column indices to a one-hot encoding SparseTensor . Example indices = [[ 0 , 1 , 4 ], [ 1 , 2 , 6 ]] dense_shape = [ 2 , 10 ] sp_one_hot = sparse_one_hot ( indices , dense_shape ) expected = tf . SparseTensor ( indices = [[ 0 , 0 ], [ 0 , 1 ], [ 0 , 4 ], [ 1 , 1 ], [ 1 , 2 ], [ 1 , 6 ]], values = [ 1 , 1 , 1 , 1 , 1 , 1 ], dense_shape = [ 2 , 10 ]) Args column_indices ( Tensor ) : a dense tensor with the indices to be active for each sample (row) num_cols ( int ) : number of columns for the one-hot encoding dtype ( tf.DType ) : the type for the output values. name ( str ) : name for this op Returns sp_tensor ( SparseTensor ) : a sparse tensor with the one hot encoding for the given indices dropout source . dropout ( tensor , noise_shape = None , random_mask = None , probability = 0.1 , scale = True , seed = None , return_mask = False , name = 'dropout' ) With probability probability , outputs 0 otherwise outputs the input element. If scale is True, the input elements are scaled up by 1 / (1-probability) so that the expected sum of the activations is unchanged. By default, each element is kept or dropped independently. If noise_shape is specified, it must be broadcastable to the shape of x , and only dimensions with noise_shape[i] == shape(x)[i] will make independent decisions. For example, if shape(x) = [k, l, m, n] and noise_shape = [k, 1, 1, n] , each batch and channel component will be kept independently and each row and column will be kept or not kept together. Args tensor ( Tensor ) : an input tensor noise_shape ( Tensor ) : A 1-D Tensor of type int32 , representing the shape for randomly generated drop flags return_mask ( bool ) : if True , returns the random mask used random_mask ( Tensor ) : a tensor used to create the random bernoulli mask probability ( float or Tensor ) : A scalar Tensor with the same type as x. The probability that each element is kept. scale ( bool ) : if true rescales the non-zero elements to 1 / (1-drop_probability) seed ( int ) : A Python integer with the random number generator seed name ( str ) : a name for this operation Returns tensor ( Tensor ) : output tensor with the same DType as the input Raises ValueError : if probability is not in [0, 1] or if x is not a floating point tensor. alpha_dropout source . alpha_dropout ( tensor , noise_shape = None , random_mask = None , probability = 0.1 , seed = None , return_mask = False , name = 'dropout' ) Alpha Dropout keeps mean and variance of inputs in order to ensure the self-normalization after dropout. Alpha dropout is proposed for Scaled Exponential Linear Units (SELUs) because it randomly sets activations to the negative saturation value rather than 0. The multiplicative noise will have standard deviation $\\sqrt{\\frac{probability}{(1-probability)}} References Self-Normalizing Neural Networks Args tensor ( Tensor ) : A floating point tensor. noise_shape ( Tensor ) : A 1-D Tensor of type int32 , representing the shape for randomly generated drop flags return_mask ( bool ) : if true, returns the random mask used random_mask ( Tensor ) : a tensor used to create the random bernoulli mask probability ( float or Tensor ) : A scalar Tensor with the same type as x. The probability that each element is kept. seed ( int ) : A Python integer with the random number generator seed name ( str ) : a name for this operation (optional) Returns result ( Tensor ) : a tensor with the same shape as the input with the dropped units set to negative values sparse_dropout source . sparse_dropout ( sp_tensor , probability = 0.2 , scale = True , seed = None , mask = None , return_mask = False , alpha = False , name = 'sparse_dropout' ) Performs a dropout on a SparseTensor . With probability keep_prob , outputs the input element scaled up by 1 / keep_prob , otherwise outputs 0 . The scaling is so that the expected sum is unchanged. Args sp_tensor ( SparseTensor ) : a sparse tensor on which the dropout is performed. mask ( Tensor ) : a binary random mask to be applied to the values of this tensor return_mask ( bool ) : if true returns the random_mask used to perform dropout (result,random_mask) probability ( float , Tensor ) : A scalar tensor with the same type as x. The probability that each element is kept. scale ( bool ) : if True rescales the input to 1 / keep_prob else simply drops without rescaling seed ( int) : A Python integer used as seed. (See TensorFlow` documentation for tf.set_random_seed for behavior.) alpha ( bool ) : if True uses alpha_dropout instead of dropout in the inputs name ( str ) : A name for this operation (optional). binary_random_mask source . binary_random_mask ( tensor , mask_probability = 0.0 , seed = None ) Creates a binary mask with the same shape as the given tensor, randomly generated from the given mask probability. Args tensor ( Tensor ) : tensor for which we would like to create a mask mask_probability ( float , Tensor ) : scalar tensor or float with probability of masking a given value seed ( int ) : seed for random number generator Returns binary_mask ( Tensor ) : a tensor with values 0 or 1 with the same shape as the input tensor to_sparse source . to_sparse ( tensor , name = 'to_sparse' ) Converts a given Tensor in a SparseTensor Example For a dense Tensor such as: tensor = [[ 1 , 0 ], [ 2 , 3 ]] this returns an op that creates the following two SparseTensor : tf . SparseTensor ( indices = [[ 0 , 0 ], [ 1 , 0 ], [ 1 , 1 ]], values = [ 1 , 2 , 3 ], dense_shape = [ 2 , 2 ]) Args tensor ( Tensor ) : a dense tensor name ( str ) : name for to_sparse op Returns sp_tensor ( SparseTensor ) : a sparse tensor with sparse index and value tensors with the non-zero entries of the given input. embedding_lookup_sparse source . embedding_lookup_sparse ( params , sp_tensor , combiner = None , max_norm = None , name = 'embedding_lookup_sparse' ) Computes embeddings for the given ids and weights. Info assumes that there is at least one id for each row in the dense tensor represented by sp_ids (i.e. there are no rows with empty features), and that all the indices of sp_ids are in canonical row-major order. It also assumes that all id values lie in the range [0, p0), where p0 is the sum of the size of params along dimension 0. Note in tensorflow's implementation, sparse gradients do not propagate through gather. Args params : A single tensor representing the complete embedding tensor, or a sp_tensor ( SparseTensor ) : N x M SparseTensor with the ids and weights where N is typically batch size and M is arbitrary. combiner : A string specifying the reduction op. Currently \"mean\", \"sqrtn\" max_norm : If not None , each embedding is clipped if its l2-norm is larger name ( str ) : op name sp_tensor: list of P tensors all of same shape except for the first dimension, representing sharded embedding tensors. Alternatively, a PartitionedVariable , created by partitioning along dimension 0. Each element must be appropriately sized for the given partition_strategy . and \"sum\" are supported. \"sum\" computes the weighted sum of the embedding results for each row. \"mean\" is the weighted sum divided by the total weight. \"sqrtn\" is the weighted sum divided by the square root of the sum of the squares of the weights. than this value, before combining. Returns tensor ( Tensor ) : dense tensor representing the combined embeddings for the sparse ids. For each row in the dense tensor represented by sp_ids , the op looks up the embeddings for all ids in that row, multiplies them by the corresponding weight, and combines these embeddings as specified. Raises TypeError : If sp_ids is not a SparseTensor , or if sp_weights is ValueError : If combiner is not one of {\"mean\", \"sqrtn\", \"sum\"}. neither None nor SparseTensor . sparse_overlap source . sparse_overlap ( sp_tensor1 , sp_tensor2 , name = 'sparse_overlap' ) sparse overlap Returns a SparseTensor where the indices of the overlapping indices in the two sparse tensors with the values of the first one. Args sp_tensor1 ( SparseTensor ) : a sparse tensor sp_tensor2 ( SparseTensor ) : another sparse tensor name ( str ) : name for sparse_overlap op Returns sp_tensor ( SparseTensor ) : sparse tensor with the overlapping indices and the values of sp_tensor1 sort_by_first source . sort_by_first ( tensor1 , tensor2 , ascending = True , name = 'sort_by_first' ) sort_by_first Sorts two tensors. Sorts the second by the changes in the first sort Args tensor1 ( Tensor ) : tensor to determine the oder by which the second is sorted tensor2 ( Tensor ) : tensor to be sorted according to the sorting of the first ascending ( Bool ) : if True sorts by ascending order of value name ( str ) : name of the op Returns tensor2 ( Tensor , Tensor ) : sorted first tensor, second tensor sorted according to the indices of the first tensor sorting ranges source . ranges ( range_sizes , name = 'ranges' ) ranges similar to concatenating multiple tf.range calls applied to each element of a given 1D tensor with range sizes. Example ranges ([ 1 , 2 , 4 ]) [ 0 , 0 , 1 , 0 , 1 , 2 , 3 ] the enums are [0] , [0,1] , [0,1,2,3] Args range_sizes ( Tensor ) : 1D tensor with range sizes name ( str ) : ranges op name Returns ranges ( Tensor ) : a 1D Tensor with tf.reduce_sum(range_sizes) dimensions grid_2d source . grid_2d ( shape , name = 'grid_2d' ) creates a tensor with a grid 2d coordinates Args shape ( Tensor ) : an Tensor of tf.int32 with a 2D shape for the grid name ( str ) : grid_2d op name Returns grid_coordinates ( Tensor ) : 2D tensor with grid coordinates gather_sparse source . gather_sparse ( sp_tensor , ids , name = 'gather_sparse' ) gather_sparse gather rows from a sparse tensor by the given ids and returns a sparse tensor Warning gathering from a SparseTensor is inefficient Example gather_sparse ( sp_tensor ,[ 1 , 1 , 4 ]) returns a [3,sp_tensor.dense_shape[-1]] SparseTensor Args sp_tensor ( SparseTensor ) : sparse tensor ids ( Tensor ) : an int tensor with the ids of the rows to be returned name ( str ) : on name Returns sp_gathered ( SparseTensor ) : a sparse tensor with the gathered rows. sparse_tile source . sparse_tile ( sp_tensor , num , name = 'sparse_tile' ) Constructs a SparseTensor by replicating the input sparse tensor num times Args sp_tensor ( SparseTensor ) : a sparse input tensor to be tiled num ( int ) : number of repetitions name ( str ) : name for the op Returns sp_tile ( SparseTensor ) : result sparse tensor pairs source . pairs ( tensor1 , tensor2 , name = 'pairs' ) Pairwise combination of elements from the two tensors. Example t1 = [[ 0 ],[ 1 ]] t2 = [ 2 , 3 , 4 ] t12 = [[ 0 , 2 ],[ 1 , 2 ],[ 0 , 3 ],[ 1 , 3 ],[ 0 , 4 ],[ 1 , 4 ]] p12 = tx . pairs ( t1 , t2 ) tf . reduce_all ( tf . equal ( p12 , t12 )) Args tensor1 ( Tensor ) : a tensor, python list, or numpy array tensor2 ( Tensor ) : a tensor, python list, or numpy array name ( str ) : name for pairs op) Returns tensor ( Tensor ) : a tensor with the pairwise combination of input tensors sparse_put source . sparse_put ( sp_tensor , sp_updates , name = 'sparse_put' ) sparse_put Changes a given tf.SparseTensor according to the updates specified in a tf.SparseTensor. Creates a new tensor where the values of the updates override the values in the original tensor. The input tensors must have the same dense_shape . Args sp_tensor ( SparseTensor ) : a sparse tensor we which to set some indices to given values sp_updates (`SparseTensor) : a SparseTensor with the indices to be changed and the respective values name ( str ) : sparse_put op name Returns sparse_tensor ( SparseTensor ) : a sparse tensor with the updated values. put source . put ( tensor , sp_updates , name = 'put' ) put Changes a given dense Tensor according to the updates specified in a SparseTensor . Creates a new Tensor where the values of the updates override the values in the original tensor. The tensor shape must be the same as the updates dense_shape . Args tensor ( Tensor ) : tensor to be updated sp_updates ( SparseTensor ) : sparse tensor with the indices to be changed and the respective values. name ( str ) : put op name Returns tensor ( Tensor ) : a tensor with the updated values. filter_nd source . filter_nd ( condition , params , name = 'filter_nd' ) filter_nd Filters a given tensor based on a condition tensor condition and params must have the same shape Args condition ( Tensor ) : a bool tensor used to filter params params ( Tensor ) : the tensor to be filtered name ( str ) : name for filter_nd op Returns sp_tensor ( SparseTensor ) : a sparse tensor with the values in params filtered according to condition","title":"ops"},{"location":"api/ops/#_1","text":"","title":""},{"location":"api/ops/#txops","text":"","title":"tx.ops"},{"location":"api/ops/#matrix_indices","text":"source . matrix_indices ( index_tensor , dtype = tf . int64 , sort_indices = True , name = 'matrix_indices' ) Transforms a batch of column indices into a batch of matrix indices Args index_tensor ( Tensor ) : a tensor with shape (b,n) with a batch of n column indices. dtype ( DType ) : the output dtype for the indices. Defaults to int64 . sort_indices ( bool ) : if True , output indices are sorted in canonical row-major order. name ( str ) : name for this op. Returns tensor ( Tensor ) : tensor with shape [b,2] for each index in the input tensor with the corresponding matrix indices","title":"matrix_indices"},{"location":"api/ops/#empty_sparse_tensor","text":"source . empty_sparse_tensor ( dense_shape , dtype = tf . float32 , name = 'empty_sp_tensor' ) Creates an empty SparseTensor Args dense_shape ( TensorShape ) : a 1-D tensor, python list, or numpy array with the output shape for the sparse tensor dtype ( DType ) : the dtype of the values for the empty tf.SparseTensor name ( str ) : a name for this operation Returns sp_tensor ( SparseTensor ) : an empty sparse tensor with a given shape","title":"empty_sparse_tensor"},{"location":"api/ops/#sparse_ones","text":"source . sparse_ones ( indices , dense_shape , dtype = tf . float32 , name = 'sparse_ones' ) Creates a new SparseTensor with the given indices having value 1 Args indices ( Tensor ) : a rank 2 tensor with the (row,column) indices for the resulting sparse tensor dense_shape ( Tensor or TensorShape ) : the output dense shape dtype ( tf.DType ) : the tensor type for the values name ( str ) : sparse_ones op Returns sp_tensor ( SparseTensor ) : a new sparse tensor with values set to 1","title":"sparse_ones"},{"location":"api/ops/#sparse_zeros","text":"source . sparse_zeros ( indices , dense_shape , dtype = tf . float32 , name = 'sparse_zeros' ) Creates a new SparseTensor with the given indices having value 0 Args indices ( Tensor ) : a rank 2 tensor with the (row,column) indices for the resulting sparse tensor dense_shape ( Tensor or TensorShape ) : the output dense shape dtype ( tf.DType ) : the tensor type for the values name ( str ) : sparse_ones op Returns sp_tensor ( SparseTensor ) : a new sparse tensor with values set to 0","title":"sparse_zeros"},{"location":"api/ops/#sparse_overlap","text":"source . sparse_overlap ( sp_tensor1 , sp_tensor2 , name = 'sparse_overlap' ) sparse overlap Returns a SparseTensor where the indices of the overlapping indices in the two sparse tensors with the values of the first one. Args sp_tensor1 ( SparseTensor ) : a sparse tensor sp_tensor2 ( SparseTensor ) : another sparse tensor name ( str ) : name for sparse_overlap op Returns sp_tensor ( SparseTensor ) : sparse tensor with the overlapping indices and the values of sp_tensor1","title":"sparse_overlap"},{"location":"api/ops/#apply_gate","text":"source . apply_gate ( tensor , gate ) Applies a gate tensor to the given input if input tensor outer dimension is a multiple of gate outer dimension we use broadcasting to apply the gate evenly across the input tensor. Example tx . apply_gate ( tf . ones ([ 1 , 4 ]),[ 1. , 0. ]) [[ 1. , 1. , 0. , 0. ]] Args tensor ( Tensor ) : an input tensor gate ( Tensor ) : float tensor that is multiplied by the input tensor. The outer dimension of the input tensor should either match the gate tensor or be a multiple of gate tensor. Returns gated ( Tensor ) : input tensor gated using the given gate weights","title":"apply_gate"},{"location":"api/ops/#sparse_indices","text":"source . sparse_indices ( sp_values , name = 'sparse_indices' ) Returns a SparseTensor with the values containing column indices for the active values on a given SparseTensor . Use Case To be used with embedding_lookup_sparse when we need two SparseTensor objects with the indices and values Args sp_values ( SparseTensor ) : a sparse tensor for which we extract the active indices. name ( str ) : name for sparse_indices op Returns sp_indices ( SparseTensor ) : a sparse tensor with the column indices","title":"sparse_indices"},{"location":"api/ops/#sparse_matrix_indices","text":"source . sparse_matrix_indices ( column_indices , num_cols , dtype = tf . float32 , name = 'sparse_one_hot' ) Transforms a batch of column indices to a one-hot encoding SparseTensor . Example indices = [[ 0 , 1 , 4 ], [ 1 , 2 , 6 ]] dense_shape = [ 2 , 10 ] sp_one_hot = sparse_one_hot ( indices , dense_shape ) expected = tf . SparseTensor ( indices = [[ 0 , 0 ], [ 0 , 1 ], [ 0 , 4 ], [ 1 , 1 ], [ 1 , 2 ], [ 1 , 6 ]], values = [ 1 , 1 , 1 , 1 , 1 , 1 ], dense_shape = [ 2 , 10 ]) Args column_indices ( Tensor ) : a dense tensor with the indices to be active for each sample (row) num_cols ( int ) : number of columns for the one-hot encoding dtype ( tf.DType ) : the type for the output values. name ( str ) : name for this op Returns sp_tensor ( SparseTensor ) : a sparse tensor with the one hot encoding for the given indices","title":"sparse_matrix_indices"},{"location":"api/ops/#dropout","text":"source . dropout ( tensor , noise_shape = None , random_mask = None , probability = 0.1 , scale = True , seed = None , return_mask = False , name = 'dropout' ) With probability probability , outputs 0 otherwise outputs the input element. If scale is True, the input elements are scaled up by 1 / (1-probability) so that the expected sum of the activations is unchanged. By default, each element is kept or dropped independently. If noise_shape is specified, it must be broadcastable to the shape of x , and only dimensions with noise_shape[i] == shape(x)[i] will make independent decisions. For example, if shape(x) = [k, l, m, n] and noise_shape = [k, 1, 1, n] , each batch and channel component will be kept independently and each row and column will be kept or not kept together. Args tensor ( Tensor ) : an input tensor noise_shape ( Tensor ) : A 1-D Tensor of type int32 , representing the shape for randomly generated drop flags return_mask ( bool ) : if True , returns the random mask used random_mask ( Tensor ) : a tensor used to create the random bernoulli mask probability ( float or Tensor ) : A scalar Tensor with the same type as x. The probability that each element is kept. scale ( bool ) : if true rescales the non-zero elements to 1 / (1-drop_probability) seed ( int ) : A Python integer with the random number generator seed name ( str ) : a name for this operation Returns tensor ( Tensor ) : output tensor with the same DType as the input Raises ValueError : if probability is not in [0, 1] or if x is not a floating point tensor.","title":"dropout"},{"location":"api/ops/#alpha_dropout","text":"source . alpha_dropout ( tensor , noise_shape = None , random_mask = None , probability = 0.1 , seed = None , return_mask = False , name = 'dropout' ) Alpha Dropout keeps mean and variance of inputs in order to ensure the self-normalization after dropout. Alpha dropout is proposed for Scaled Exponential Linear Units (SELUs) because it randomly sets activations to the negative saturation value rather than 0. The multiplicative noise will have standard deviation $\\sqrt{\\frac{probability}{(1-probability)}} References Self-Normalizing Neural Networks Args tensor ( Tensor ) : A floating point tensor. noise_shape ( Tensor ) : A 1-D Tensor of type int32 , representing the shape for randomly generated drop flags return_mask ( bool ) : if true, returns the random mask used random_mask ( Tensor ) : a tensor used to create the random bernoulli mask probability ( float or Tensor ) : A scalar Tensor with the same type as x. The probability that each element is kept. seed ( int ) : A Python integer with the random number generator seed name ( str ) : a name for this operation (optional) Returns result ( Tensor ) : a tensor with the same shape as the input with the dropped units set to negative values","title":"alpha_dropout"},{"location":"api/ops/#sparse_dropout","text":"source . sparse_dropout ( sp_tensor , probability = 0.2 , scale = True , seed = None , mask = None , return_mask = False , alpha = False , name = 'sparse_dropout' ) Performs a dropout on a SparseTensor . With probability keep_prob , outputs the input element scaled up by 1 / keep_prob , otherwise outputs 0 . The scaling is so that the expected sum is unchanged. Args sp_tensor ( SparseTensor ) : a sparse tensor on which the dropout is performed. mask ( Tensor ) : a binary random mask to be applied to the values of this tensor return_mask ( bool ) : if true returns the random_mask used to perform dropout (result,random_mask) probability ( float , Tensor ) : A scalar tensor with the same type as x. The probability that each element is kept. scale ( bool ) : if True rescales the input to 1 / keep_prob else simply drops without rescaling seed ( int) : A Python integer used as seed. (See TensorFlow` documentation for tf.set_random_seed for behavior.) alpha ( bool ) : if True uses alpha_dropout instead of dropout in the inputs name ( str ) : A name for this operation (optional).","title":"sparse_dropout"},{"location":"api/ops/#binary_random_mask","text":"source . binary_random_mask ( tensor , mask_probability = 0.0 , seed = None ) Creates a binary mask with the same shape as the given tensor, randomly generated from the given mask probability. Args tensor ( Tensor ) : tensor for which we would like to create a mask mask_probability ( float , Tensor ) : scalar tensor or float with probability of masking a given value seed ( int ) : seed for random number generator Returns binary_mask ( Tensor ) : a tensor with values 0 or 1 with the same shape as the input tensor","title":"binary_random_mask"},{"location":"api/ops/#to_sparse","text":"source . to_sparse ( tensor , name = 'to_sparse' ) Converts a given Tensor in a SparseTensor Example For a dense Tensor such as: tensor = [[ 1 , 0 ], [ 2 , 3 ]] this returns an op that creates the following two SparseTensor : tf . SparseTensor ( indices = [[ 0 , 0 ], [ 1 , 0 ], [ 1 , 1 ]], values = [ 1 , 2 , 3 ], dense_shape = [ 2 , 2 ]) Args tensor ( Tensor ) : a dense tensor name ( str ) : name for to_sparse op Returns sp_tensor ( SparseTensor ) : a sparse tensor with sparse index and value tensors with the non-zero entries of the given input.","title":"to_sparse"},{"location":"api/ops/#embedding_lookup_sparse","text":"source . embedding_lookup_sparse ( params , sp_tensor , combiner = None , max_norm = None , name = 'embedding_lookup_sparse' ) Computes embeddings for the given ids and weights. Info assumes that there is at least one id for each row in the dense tensor represented by sp_ids (i.e. there are no rows with empty features), and that all the indices of sp_ids are in canonical row-major order. It also assumes that all id values lie in the range [0, p0), where p0 is the sum of the size of params along dimension 0. Note in tensorflow's implementation, sparse gradients do not propagate through gather. Args params : A single tensor representing the complete embedding tensor, or a sp_tensor ( SparseTensor ) : N x M SparseTensor with the ids and weights where N is typically batch size and M is arbitrary. combiner : A string specifying the reduction op. Currently \"mean\", \"sqrtn\" max_norm : If not None , each embedding is clipped if its l2-norm is larger name ( str ) : op name sp_tensor: list of P tensors all of same shape except for the first dimension, representing sharded embedding tensors. Alternatively, a PartitionedVariable , created by partitioning along dimension 0. Each element must be appropriately sized for the given partition_strategy . and \"sum\" are supported. \"sum\" computes the weighted sum of the embedding results for each row. \"mean\" is the weighted sum divided by the total weight. \"sqrtn\" is the weighted sum divided by the square root of the sum of the squares of the weights. than this value, before combining. Returns tensor ( Tensor ) : dense tensor representing the combined embeddings for the sparse ids. For each row in the dense tensor represented by sp_ids , the op looks up the embeddings for all ids in that row, multiplies them by the corresponding weight, and combines these embeddings as specified. Raises TypeError : If sp_ids is not a SparseTensor , or if sp_weights is ValueError : If combiner is not one of {\"mean\", \"sqrtn\", \"sum\"}. neither None nor SparseTensor .","title":"embedding_lookup_sparse"},{"location":"api/ops/#sparse_overlap_1","text":"source . sparse_overlap ( sp_tensor1 , sp_tensor2 , name = 'sparse_overlap' ) sparse overlap Returns a SparseTensor where the indices of the overlapping indices in the two sparse tensors with the values of the first one. Args sp_tensor1 ( SparseTensor ) : a sparse tensor sp_tensor2 ( SparseTensor ) : another sparse tensor name ( str ) : name for sparse_overlap op Returns sp_tensor ( SparseTensor ) : sparse tensor with the overlapping indices and the values of sp_tensor1","title":"sparse_overlap"},{"location":"api/ops/#sort_by_first","text":"source . sort_by_first ( tensor1 , tensor2 , ascending = True , name = 'sort_by_first' ) sort_by_first Sorts two tensors. Sorts the second by the changes in the first sort Args tensor1 ( Tensor ) : tensor to determine the oder by which the second is sorted tensor2 ( Tensor ) : tensor to be sorted according to the sorting of the first ascending ( Bool ) : if True sorts by ascending order of value name ( str ) : name of the op Returns tensor2 ( Tensor , Tensor ) : sorted first tensor, second tensor sorted according to the indices of the first tensor sorting","title":"sort_by_first"},{"location":"api/ops/#ranges","text":"source . ranges ( range_sizes , name = 'ranges' ) ranges similar to concatenating multiple tf.range calls applied to each element of a given 1D tensor with range sizes. Example ranges ([ 1 , 2 , 4 ]) [ 0 , 0 , 1 , 0 , 1 , 2 , 3 ] the enums are [0] , [0,1] , [0,1,2,3] Args range_sizes ( Tensor ) : 1D tensor with range sizes name ( str ) : ranges op name Returns ranges ( Tensor ) : a 1D Tensor with tf.reduce_sum(range_sizes) dimensions","title":"ranges"},{"location":"api/ops/#grid_2d","text":"source . grid_2d ( shape , name = 'grid_2d' ) creates a tensor with a grid 2d coordinates Args shape ( Tensor ) : an Tensor of tf.int32 with a 2D shape for the grid name ( str ) : grid_2d op name Returns grid_coordinates ( Tensor ) : 2D tensor with grid coordinates","title":"grid_2d"},{"location":"api/ops/#gather_sparse","text":"source . gather_sparse ( sp_tensor , ids , name = 'gather_sparse' ) gather_sparse gather rows from a sparse tensor by the given ids and returns a sparse tensor Warning gathering from a SparseTensor is inefficient Example gather_sparse ( sp_tensor ,[ 1 , 1 , 4 ]) returns a [3,sp_tensor.dense_shape[-1]] SparseTensor Args sp_tensor ( SparseTensor ) : sparse tensor ids ( Tensor ) : an int tensor with the ids of the rows to be returned name ( str ) : on name Returns sp_gathered ( SparseTensor ) : a sparse tensor with the gathered rows.","title":"gather_sparse"},{"location":"api/ops/#sparse_tile","text":"source . sparse_tile ( sp_tensor , num , name = 'sparse_tile' ) Constructs a SparseTensor by replicating the input sparse tensor num times Args sp_tensor ( SparseTensor ) : a sparse input tensor to be tiled num ( int ) : number of repetitions name ( str ) : name for the op Returns sp_tile ( SparseTensor ) : result sparse tensor","title":"sparse_tile"},{"location":"api/ops/#pairs","text":"source . pairs ( tensor1 , tensor2 , name = 'pairs' ) Pairwise combination of elements from the two tensors. Example t1 = [[ 0 ],[ 1 ]] t2 = [ 2 , 3 , 4 ] t12 = [[ 0 , 2 ],[ 1 , 2 ],[ 0 , 3 ],[ 1 , 3 ],[ 0 , 4 ],[ 1 , 4 ]] p12 = tx . pairs ( t1 , t2 ) tf . reduce_all ( tf . equal ( p12 , t12 )) Args tensor1 ( Tensor ) : a tensor, python list, or numpy array tensor2 ( Tensor ) : a tensor, python list, or numpy array name ( str ) : name for pairs op) Returns tensor ( Tensor ) : a tensor with the pairwise combination of input tensors","title":"pairs"},{"location":"api/ops/#sparse_put","text":"source . sparse_put ( sp_tensor , sp_updates , name = 'sparse_put' ) sparse_put Changes a given tf.SparseTensor according to the updates specified in a tf.SparseTensor. Creates a new tensor where the values of the updates override the values in the original tensor. The input tensors must have the same dense_shape . Args sp_tensor ( SparseTensor ) : a sparse tensor we which to set some indices to given values sp_updates (`SparseTensor) : a SparseTensor with the indices to be changed and the respective values name ( str ) : sparse_put op name Returns sparse_tensor ( SparseTensor ) : a sparse tensor with the updated values.","title":"sparse_put"},{"location":"api/ops/#put","text":"source . put ( tensor , sp_updates , name = 'put' ) put Changes a given dense Tensor according to the updates specified in a SparseTensor . Creates a new Tensor where the values of the updates override the values in the original tensor. The tensor shape must be the same as the updates dense_shape . Args tensor ( Tensor ) : tensor to be updated sp_updates ( SparseTensor ) : sparse tensor with the indices to be changed and the respective values. name ( str ) : put op name Returns tensor ( Tensor ) : a tensor with the updated values.","title":"put"},{"location":"api/ops/#filter_nd","text":"source . filter_nd ( condition , params , name = 'filter_nd' ) filter_nd Filters a given tensor based on a condition tensor condition and params must have the same shape Args condition ( Tensor ) : a bool tensor used to filter params params ( Tensor ) : the tensor to be filtered name ( str ) : name for filter_nd op Returns sp_tensor ( SparseTensor ) : a sparse tensor with the values in params filtered according to condition","title":"filter_nd"},{"location":"api/layers/","text":"Classes class LayerState class LayerConfig class Layer class Lambda class Input class Linear class Module class BaseRNNCell class RNN class RNNCell class Lookup class GRUCell class LSTMCell class Activation class MHAttention Functions layer","title":"Overview"},{"location":"api/layers/#classes","text":"class LayerState class LayerConfig class Layer class Lambda class Input class Linear class Module class BaseRNNCell class RNN class RNNCell class Lookup class GRUCell class LSTMCell class Activation class MHAttention","title":"Classes"},{"location":"api/layers/#functions","text":"layer","title":"Functions"},{"location":"api/layers/Activation/","text":"Activation source Activation ( layer , fn = tx . identity , name = \"activation\" , ** kwargs ) Applies the given function the the output of the input Layer . Warning if the input layer outputs a SparseTensor , this is converted to a dense Tensor first. Args input_layer ( Layer ) : input layer to which the activation function is applied fn : a function that produces a Tensor and can be called on the tensor produced by the input layer name : the layer name kwargs : the keyword arguments passed to the given fn function Methods: .compute_shape source . compute_shape () .compute source . compute ( input_tensor ) .reuse_with source . reuse_with ( input_layer , name = None )","title":"Activation"},{"location":"api/layers/Activation/#_1","text":"","title":""},{"location":"api/layers/Activation/#activation","text":"source Activation ( layer , fn = tx . identity , name = \"activation\" , ** kwargs ) Applies the given function the the output of the input Layer . Warning if the input layer outputs a SparseTensor , this is converted to a dense Tensor first. Args input_layer ( Layer ) : input layer to which the activation function is applied fn : a function that produces a Tensor and can be called on the tensor produced by the input layer name : the layer name kwargs : the keyword arguments passed to the given fn function Methods:","title":"Activation"},{"location":"api/layers/Activation/#compute_shape","text":"source . compute_shape ()","title":".compute_shape"},{"location":"api/layers/Activation/#compute","text":"source . compute ( input_tensor )","title":".compute"},{"location":"api/layers/Activation/#reuse_with","text":"source . reuse_with ( input_layer , name = None )","title":".reuse_with"},{"location":"api/layers/Input/","text":"Input source Input ( init_value = None , n_units = None , constant = False , sparse = False , n_active : Optional [ int ] = None , shape = None , dtype = None , cast = True , name = 'input' ) Input Layer An Input layer defines constant or dynamic inputs of a neural network model in TensorX. An input layer has no inputs and is usable as a placeholder for Tensorflow Tensor or SparseTensor objects. An Input layer is stateful , which means that it can hold and output a given value until this value is changed. import tensorx as tx # assumes a shape [None,2] x = tx . Input ( n_units = 2 , constant = False ) v1 = x () [[ 0 , 0 ]] x . value = tf . ones ([ 2 , 2 ]) v2 = x () [[ 1 , 1 ], [ 1 , 1 ]] x . value = tf . ones ([ 2 , 3 ]) # throws an exception because it expects a shape [None,2] x2 = tx . Input ( n_units = 2 , constant = True ) x2 () [[ 0 , 0 ]] x2 . value = tf . ones ([ 2 , 2 ]) # throws ValueError: Cannot set the value of a constant Input Layer Info when n_active is provided, Input layers are interpreted as representing binary sparse (one-hot)[https://en.wikipedia.org/wiki/One-hot] encoding and expects it's values to be of type tf.int64 . both Linear and Lookup layers are compatible with Input layers that output SparseTensor objects, representing one-hot encodings of categorical inputs. SparseTensor value can be passed as an initial value. Args init_value ( Tensor ) : initial value for Input layer, if given, it determines n_units n_units ( int or None ) : number of output units for this layer. n_active : number of active units <= n_units. If given, input is a Tensor with col indices sparse ( bool ) : if true, expects the input value to be a SparseTensor . shape ( TensorShape ) : expected input shape dtype ( tf.Dtype ) : type for input values. constant : if true, input value cannot be changed after Input is initialized. name (str) : layer name cast (bool) : if True tries to cast the input to the given dtype on value set Attributes value ( Union[Tensor , SparseTensor ]) : if constant=True value cannot be set and an exception is raised Methods: .compute_shape source . compute_shape () .init_state source . init_state () .compute source . compute () .reuse_with source . reuse_with ( * layers , name = None )","title":"Input"},{"location":"api/layers/Input/#_1","text":"","title":""},{"location":"api/layers/Input/#input","text":"source Input ( init_value = None , n_units = None , constant = False , sparse = False , n_active : Optional [ int ] = None , shape = None , dtype = None , cast = True , name = 'input' ) Input Layer An Input layer defines constant or dynamic inputs of a neural network model in TensorX. An input layer has no inputs and is usable as a placeholder for Tensorflow Tensor or SparseTensor objects. An Input layer is stateful , which means that it can hold and output a given value until this value is changed. import tensorx as tx # assumes a shape [None,2] x = tx . Input ( n_units = 2 , constant = False ) v1 = x () [[ 0 , 0 ]] x . value = tf . ones ([ 2 , 2 ]) v2 = x () [[ 1 , 1 ], [ 1 , 1 ]] x . value = tf . ones ([ 2 , 3 ]) # throws an exception because it expects a shape [None,2] x2 = tx . Input ( n_units = 2 , constant = True ) x2 () [[ 0 , 0 ]] x2 . value = tf . ones ([ 2 , 2 ]) # throws ValueError: Cannot set the value of a constant Input Layer Info when n_active is provided, Input layers are interpreted as representing binary sparse (one-hot)[https://en.wikipedia.org/wiki/One-hot] encoding and expects it's values to be of type tf.int64 . both Linear and Lookup layers are compatible with Input layers that output SparseTensor objects, representing one-hot encodings of categorical inputs. SparseTensor value can be passed as an initial value. Args init_value ( Tensor ) : initial value for Input layer, if given, it determines n_units n_units ( int or None ) : number of output units for this layer. n_active : number of active units <= n_units. If given, input is a Tensor with col indices sparse ( bool ) : if true, expects the input value to be a SparseTensor . shape ( TensorShape ) : expected input shape dtype ( tf.Dtype ) : type for input values. constant : if true, input value cannot be changed after Input is initialized. name (str) : layer name cast (bool) : if True tries to cast the input to the given dtype on value set Attributes value ( Union[Tensor , SparseTensor ]) : if constant=True value cannot be set and an exception is raised Methods:","title":"Input"},{"location":"api/layers/Input/#compute_shape","text":"source . compute_shape ()","title":".compute_shape"},{"location":"api/layers/Input/#init_state","text":"source . init_state ()","title":".init_state"},{"location":"api/layers/Input/#compute","text":"source . compute ()","title":".compute"},{"location":"api/layers/Input/#reuse_with","text":"source . reuse_with ( * layers , name = None )","title":".reuse_with"},{"location":"api/layers/Lambda/","text":"Lambda source Lambda ( * layers , fn , n_units = None , var_list = None , dtype = None , shape = None , name = 'lambda' , apply_to_layer = False , ** kwargs ) Custom Function Layer Attributes tensor : the tensor to be wrapped by this layer var_list : if vars are involved in the output tensor, they can be specified here n_units : number of units for this layer, batch_size : Optional batch size for this layer apply_to_layer ( bool ) : if False applies the function to the tensors otherwise applies to the layer and will be listed in variables Creates a layer from a given tensor that one can then integrate with other layers Args layers ( Sequence[Layer] ) : sequence of input layers n_units ( int ) : number of output units (outer dimension) var_list ( List[tf.Variable] ) : list of variables used by this Layer dtype ( tf.Dtype ) : tensor data type name ( str ) : layer name apply_to_layer ( bool ) : if True applies function to a layer object else applies the function to the output of previous layer.compute Methods: .init_state source . init_state () .compute_shape source . compute_shape () .compute source . compute ( * input_tensors ) .reuse_with source . reuse_with ( * layers , name = None )","title":"Lambda"},{"location":"api/layers/Lambda/#_1","text":"","title":""},{"location":"api/layers/Lambda/#lambda","text":"source Lambda ( * layers , fn , n_units = None , var_list = None , dtype = None , shape = None , name = 'lambda' , apply_to_layer = False , ** kwargs ) Custom Function Layer Attributes tensor : the tensor to be wrapped by this layer var_list : if vars are involved in the output tensor, they can be specified here n_units : number of units for this layer, batch_size : Optional batch size for this layer apply_to_layer ( bool ) : if False applies the function to the tensors otherwise applies to the layer and will be listed in variables Creates a layer from a given tensor that one can then integrate with other layers Args layers ( Sequence[Layer] ) : sequence of input layers n_units ( int ) : number of output units (outer dimension) var_list ( List[tf.Variable] ) : list of variables used by this Layer dtype ( tf.Dtype ) : tensor data type name ( str ) : layer name apply_to_layer ( bool ) : if True applies function to a layer object else applies the function to the output of previous layer.compute Methods:","title":"Lambda"},{"location":"api/layers/Lambda/#init_state","text":"source . init_state ()","title":".init_state"},{"location":"api/layers/Lambda/#compute_shape","text":"source . compute_shape ()","title":".compute_shape"},{"location":"api/layers/Lambda/#compute","text":"source . compute ( * input_tensors )","title":".compute"},{"location":"api/layers/Lambda/#reuse_with","text":"source . reuse_with ( * layers , name = None )","title":".reuse_with"},{"location":"api/layers/Layer/","text":"Layer source Layer ( inputs , n_units , shape = None , dtype = None , name = 'layer' , ** kwargs ) Layer Base Class Passing Attributes All keyword attributes passed to this constructor will be set as instance attributes so a common case for the implementing class might be: class CustomLayer ( Layer ): def __init__ ( layer , n_units , param = 1 ): # for the linter self . param = param # ... super (). __init__ ( inputs = layer , n_units = n_units , param = value ) in = tx . Input ( 10 ) y = CustomLayer ( in , 4 , param = 2 ) assert y . param == 2 Attributes inputs ( Sequence[Layer] ) : a list of input nodes for the current layer n_units : the number of units for the current layer (last dim) name ( str ) : name to be used for the layer scope config ( LayerConfig ) : a layer configuration with the arguments used in the current layer instance scoped_name ( str ) : layer full scope name Args inputs ( Sequence[Layer] ) : a single layer,a list of input layers, or None if no inputs are required n_units ( int ) : dimension of input vector (dimension of columns in case batch_size != None dtype ( DType ) : dtype for the current layer output shape ( TensorShape ) : output shape. If not None overrides compute_shape name ( str ) : layer name (used to nam the placeholder) kwargs ( Any ) : other keyword args to be set as instance attributes Methods: .compute_shape source . compute_shape () called before init_state Returns shape ( tf.TensorShape ) : best guess for the output shape of the layer .init_state source . init_state () init_state meant to be overriden in subclasses Creates an empty LayerState object Overriding init_state() Classes implementing Layer should override this method def init_state( self ): state = super() . init_state() # or state = LayerState() state . var1 = var1 state . var2 = var2 return state Layer will take this state object and add var1 and var2 to attributes. Returns state ( LayerState ) : current layer state object .compute source . compute ( * args ) .as_function source . as_function ( name = 'layer_function' , compile = False ) returns a python function of a Tensorflow compiled graph as a callable Note This returns the entire graph as a function that terminates on this layer. If you want the function for this layer alone just get the tf.function(layer.compute) Args name ( str ) : function name to be returned compile ( bool ) : if True, returns a Tensorflow compile graph as a callable else, returns a python function. Returns fn ( Callable ) : either a Tensorflow static graph or a python callable function. .reuse_with source . reuse_with ( * layers , ** kwargs )","title":"Layer"},{"location":"api/layers/Layer/#_1","text":"","title":""},{"location":"api/layers/Layer/#layer","text":"source Layer ( inputs , n_units , shape = None , dtype = None , name = 'layer' , ** kwargs ) Layer Base Class Passing Attributes All keyword attributes passed to this constructor will be set as instance attributes so a common case for the implementing class might be: class CustomLayer ( Layer ): def __init__ ( layer , n_units , param = 1 ): # for the linter self . param = param # ... super (). __init__ ( inputs = layer , n_units = n_units , param = value ) in = tx . Input ( 10 ) y = CustomLayer ( in , 4 , param = 2 ) assert y . param == 2 Attributes inputs ( Sequence[Layer] ) : a list of input nodes for the current layer n_units : the number of units for the current layer (last dim) name ( str ) : name to be used for the layer scope config ( LayerConfig ) : a layer configuration with the arguments used in the current layer instance scoped_name ( str ) : layer full scope name Args inputs ( Sequence[Layer] ) : a single layer,a list of input layers, or None if no inputs are required n_units ( int ) : dimension of input vector (dimension of columns in case batch_size != None dtype ( DType ) : dtype for the current layer output shape ( TensorShape ) : output shape. If not None overrides compute_shape name ( str ) : layer name (used to nam the placeholder) kwargs ( Any ) : other keyword args to be set as instance attributes Methods:","title":"Layer"},{"location":"api/layers/Layer/#compute_shape","text":"source . compute_shape () called before init_state Returns shape ( tf.TensorShape ) : best guess for the output shape of the layer","title":".compute_shape"},{"location":"api/layers/Layer/#init_state","text":"source . init_state () init_state meant to be overriden in subclasses Creates an empty LayerState object Overriding init_state() Classes implementing Layer should override this method def init_state( self ): state = super() . init_state() # or state = LayerState() state . var1 = var1 state . var2 = var2 return state Layer will take this state object and add var1 and var2 to attributes. Returns state ( LayerState ) : current layer state object","title":".init_state"},{"location":"api/layers/Layer/#compute","text":"source . compute ( * args )","title":".compute"},{"location":"api/layers/Layer/#as_function","text":"source . as_function ( name = 'layer_function' , compile = False ) returns a python function of a Tensorflow compiled graph as a callable Note This returns the entire graph as a function that terminates on this layer. If you want the function for this layer alone just get the tf.function(layer.compute) Args name ( str ) : function name to be returned compile ( bool ) : if True, returns a Tensorflow compile graph as a callable else, returns a python function. Returns fn ( Callable ) : either a Tensorflow static graph or a python callable function.","title":".as_function"},{"location":"api/layers/Layer/#reuse_with","text":"source . reuse_with ( * layers , ** kwargs )","title":".reuse_with"},{"location":"api/layers/Linear/","text":"Linear source Linear ( input_layer : Layer , n_units , shape = None add_bias = True ) Fully connected layer that implements a linear transformation of the form f(x) = Wx + b f(x) = Wx + b Args input_layer ( Layer ) : input layer or a value convertible to Layer n_units ( int ) : output dim weights_shape : weights shape, needed if n_units and input_layer.n_units is not known. weight_init ( Callable ) : weights (W) initializer function bias_init ( Callable ) : bias initializer function weights ( tf.Variable ) : variable to be used as linear weights bias ( tf.Variable ) : variable to be used as a bias add_bias (`bool) : if True, this layers becomes an affine transformation layer xW+b transpose_weights ( bool ) : if True , transposes the weights sparse_weights ( bool ) : if True indicates we are using a sparse tensor instead of a tf.Variable for weights weight_norm ( bool ) : if True weights are normalised dtype ( tf.DType ) : type for layer variables name ( str ) : layer name share_state_with ( Linear or None ) : Linear layer with which we wish to share the state Methods: .compute_shape source . compute_shape () .init_state source . init_state () .compute source . compute ( input_tensor ) .reuse_with source . reuse_with ( input_layer , name = None , transpose_weights = None , sparse_weights = None , shape = None ) Reuses the current layer on a different input.","title":"Linear"},{"location":"api/layers/Linear/#_1","text":"","title":""},{"location":"api/layers/Linear/#linear","text":"source Linear ( input_layer : Layer , n_units , shape = None add_bias = True ) Fully connected layer that implements a linear transformation of the form f(x) = Wx + b f(x) = Wx + b Args input_layer ( Layer ) : input layer or a value convertible to Layer n_units ( int ) : output dim weights_shape : weights shape, needed if n_units and input_layer.n_units is not known. weight_init ( Callable ) : weights (W) initializer function bias_init ( Callable ) : bias initializer function weights ( tf.Variable ) : variable to be used as linear weights bias ( tf.Variable ) : variable to be used as a bias add_bias (`bool) : if True, this layers becomes an affine transformation layer xW+b transpose_weights ( bool ) : if True , transposes the weights sparse_weights ( bool ) : if True indicates we are using a sparse tensor instead of a tf.Variable for weights weight_norm ( bool ) : if True weights are normalised dtype ( tf.DType ) : type for layer variables name ( str ) : layer name share_state_with ( Linear or None ) : Linear layer with which we wish to share the state Methods:","title":"Linear"},{"location":"api/layers/Linear/#compute_shape","text":"source . compute_shape ()","title":".compute_shape"},{"location":"api/layers/Linear/#init_state","text":"source . init_state ()","title":".init_state"},{"location":"api/layers/Linear/#compute","text":"source . compute ( input_tensor )","title":".compute"},{"location":"api/layers/Linear/#reuse_with","text":"source . reuse_with ( input_layer , name = None , transpose_weights = None , sparse_weights = None , shape = None ) Reuses the current layer on a different input.","title":".reuse_with"},{"location":"api/layers/Lookup/","text":"Lookup source Lookup ( input_layer , seq_size , embedding_shape , weight_init = glorot_uniform_init (), batch_size = None , add_bias = False , bias_init = tf . initializers . zeros (), bias = None , weights = None , shape = None , dtype = tf . float32 , name = 'lookup' , share_state_with = None , batch_padding = True ) A Lookup or Embeddings layer that gathers rows of a given parameter table given integer indices. Similar to the embedding_lookup operation from TensorFlow or the Embedding layer from Keras with added functionality. Note If a SparseTensor is passed as input, Lookup outputs one vector per row of the SparseTensor . If an exact batch_size is given the aggregation and padding is done based on this batch_size. If we want to lookup a batch of 2 sequences of 4 elements encoded in a SparseTensor , this should have the shape (4*batch_size,d) where batch_size=2 and d is the input n_units . Args input_layer ( Layer ) : an Input or other Layer representing indices for the lookup seq_size ( int ) : size of the sequence to be looked-up weight_init ( Callable[tf.Tensor] ) : embedding table initializer embedding_shape ( tf.TensorShape ) : lookup table shape batch_size ( int or None) : number of sequences to be looked up, if not None, will force a padding up to the specified batch_size. add_bias ( bool ) : if True adds a bias to the lookup output. bias ( tf.Tensor or tf.Variable ) : optionally pass bias value to the lookup operator weights ( tf.Tensor or tf.Variable ) : optional lookup table value shape : ( tf.TensorShape ): expected output shape for the lookup. overrides lookup.shape inference dtype ( tf.DType ) : output data type name ( str ) : layer name share_state_with ( Lookup ) : a Lookup layer with which this layer shares its state batch_padding ( bool ) : if True, pads the output according to seq_size and given (or inferred) batch_size Returns embeddings ( Tensor ) : output tensor Methods: .compute_shape source . compute_shape () .init_state source . init_state () .compute source . compute ( input_tensor ) .as_concat source . as_concat () concatenates the sequence produced by a lookup and returns the current lookup viewed as a concat sequence layer Returns seq_concat ( Wrap ) : a SeqConcat layer as a view for the Lookup layer .permute_batch_time source . permute_batch_time () .reuse_with source . reuse_with ( input_layer , name = None ) Reuses the current layer on a different input. Uses the variables in this layer to create a new Layer instance with a different input_layer Args input_layer : a `Lookup Layer name : name for the new Layer Return: Layer : a new layer with shared variables with the current layer.","title":"Lookup"},{"location":"api/layers/Lookup/#_1","text":"","title":""},{"location":"api/layers/Lookup/#lookup","text":"source Lookup ( input_layer , seq_size , embedding_shape , weight_init = glorot_uniform_init (), batch_size = None , add_bias = False , bias_init = tf . initializers . zeros (), bias = None , weights = None , shape = None , dtype = tf . float32 , name = 'lookup' , share_state_with = None , batch_padding = True ) A Lookup or Embeddings layer that gathers rows of a given parameter table given integer indices. Similar to the embedding_lookup operation from TensorFlow or the Embedding layer from Keras with added functionality. Note If a SparseTensor is passed as input, Lookup outputs one vector per row of the SparseTensor . If an exact batch_size is given the aggregation and padding is done based on this batch_size. If we want to lookup a batch of 2 sequences of 4 elements encoded in a SparseTensor , this should have the shape (4*batch_size,d) where batch_size=2 and d is the input n_units . Args input_layer ( Layer ) : an Input or other Layer representing indices for the lookup seq_size ( int ) : size of the sequence to be looked-up weight_init ( Callable[tf.Tensor] ) : embedding table initializer embedding_shape ( tf.TensorShape ) : lookup table shape batch_size ( int or None) : number of sequences to be looked up, if not None, will force a padding up to the specified batch_size. add_bias ( bool ) : if True adds a bias to the lookup output. bias ( tf.Tensor or tf.Variable ) : optionally pass bias value to the lookup operator weights ( tf.Tensor or tf.Variable ) : optional lookup table value shape : ( tf.TensorShape ): expected output shape for the lookup. overrides lookup.shape inference dtype ( tf.DType ) : output data type name ( str ) : layer name share_state_with ( Lookup ) : a Lookup layer with which this layer shares its state batch_padding ( bool ) : if True, pads the output according to seq_size and given (or inferred) batch_size Returns embeddings ( Tensor ) : output tensor Methods:","title":"Lookup"},{"location":"api/layers/Lookup/#compute_shape","text":"source . compute_shape ()","title":".compute_shape"},{"location":"api/layers/Lookup/#init_state","text":"source . init_state ()","title":".init_state"},{"location":"api/layers/Lookup/#compute","text":"source . compute ( input_tensor )","title":".compute"},{"location":"api/layers/Lookup/#as_concat","text":"source . as_concat () concatenates the sequence produced by a lookup and returns the current lookup viewed as a concat sequence layer Returns seq_concat ( Wrap ) : a SeqConcat layer as a view for the Lookup layer","title":".as_concat"},{"location":"api/layers/Lookup/#permute_batch_time","text":"source . permute_batch_time ()","title":".permute_batch_time"},{"location":"api/layers/Lookup/#reuse_with","text":"source . reuse_with ( input_layer , name = None ) Reuses the current layer on a different input. Uses the variables in this layer to create a new Layer instance with a different input_layer Args input_layer : a `Lookup Layer name : name for the new Layer Return: Layer : a new layer with shared variables with the current layer.","title":".reuse_with"},{"location":"api/layers/MHAttention/","text":"MHAttention source MHAttention ( query , key , value , n_units = None , n_heads = 1 , attention_fn = tf . nn . softmax , causality = False , attention_dropout = 0.0 , regularized = False , name = 'attention' , share_state_with = None ) Scaled Dot Product MultiHead Attention Layer (Q,K,V): Encodes representation of the input as a set of key-value pairs, (K,V), both of dimension n (input sequence length); in the context of sequence-to-sequence models, both keys and values are the encoder hidden states. In the decoder, the previous output is compressed into a query (Q of dimension m) Args n_units : output number of units, each attention head has n_units // n_head units query: key: value: Methods: .compute_shape source . compute_shape () .init_state source . init_state () .compute source . compute ( * input_tensors ) .reuse_with source . reuse_with ( query , key , value , regularized = None , causality = None , name = None )","title":"MHAttention"},{"location":"api/layers/MHAttention/#_1","text":"","title":""},{"location":"api/layers/MHAttention/#mhattention","text":"source MHAttention ( query , key , value , n_units = None , n_heads = 1 , attention_fn = tf . nn . softmax , causality = False , attention_dropout = 0.0 , regularized = False , name = 'attention' , share_state_with = None ) Scaled Dot Product MultiHead Attention Layer (Q,K,V): Encodes representation of the input as a set of key-value pairs, (K,V), both of dimension n (input sequence length); in the context of sequence-to-sequence models, both keys and values are the encoder hidden states. In the decoder, the previous output is compressed into a query (Q of dimension m) Args n_units : output number of units, each attention head has n_units // n_head units query: key: value: Methods:","title":"MHAttention"},{"location":"api/layers/MHAttention/#compute_shape","text":"source . compute_shape ()","title":".compute_shape"},{"location":"api/layers/MHAttention/#init_state","text":"source . init_state ()","title":".init_state"},{"location":"api/layers/MHAttention/#compute","text":"source . compute ( * input_tensors )","title":".compute"},{"location":"api/layers/MHAttention/#reuse_with","text":"source . reuse_with ( query , key , value , regularized = None , causality = None , name = None )","title":".reuse_with"},{"location":"api/layers/Module/","text":"Module source Module ( inputs , output = None , dependencies = None , name = 'module' ) Module Layer Creates a single Layer object from an existing graph defined between a set of inputs and a single output layer. The resulting object can be reused with other inputs just like any other Layer . Note The difference between a Module and a Graph is that a Module has a single output, and can be used as a Layer . A layer graph is a utility to group multiple layers into a single function \u2014possibly with multiple outputs. Raises ValueError : is raised if an input layer does not connect to the output. A Module needs to be a self-contained layer graph. Args inputs ( Layer or List[Layer] ) : one or more input layers output ( Layer or None ) : output layer; if no inputs are passed, it follows the graph backwards from the output Methods: .compute_shape source . compute_shape () .init_state source . init_state () .compute source . compute ( * input_tensors ) .reuse_with source . reuse_with ( * inputs , name = None )","title":"Module"},{"location":"api/layers/Module/#_1","text":"","title":""},{"location":"api/layers/Module/#module","text":"source Module ( inputs , output = None , dependencies = None , name = 'module' ) Module Layer Creates a single Layer object from an existing graph defined between a set of inputs and a single output layer. The resulting object can be reused with other inputs just like any other Layer . Note The difference between a Module and a Graph is that a Module has a single output, and can be used as a Layer . A layer graph is a utility to group multiple layers into a single function \u2014possibly with multiple outputs. Raises ValueError : is raised if an input layer does not connect to the output. A Module needs to be a self-contained layer graph. Args inputs ( Layer or List[Layer] ) : one or more input layers output ( Layer or None ) : output layer; if no inputs are passed, it follows the graph backwards from the output Methods:","title":"Module"},{"location":"api/layers/Module/#compute_shape","text":"source . compute_shape ()","title":".compute_shape"},{"location":"api/layers/Module/#init_state","text":"source . init_state ()","title":".init_state"},{"location":"api/layers/Module/#compute","text":"source . compute ( * input_tensors )","title":".compute"},{"location":"api/layers/Module/#reuse_with","text":"source . reuse_with ( * inputs , name = None )","title":".reuse_with"},{"location":"api/layers/rnn/BaseRNNCell/","text":"BaseRNNCell source BaseRNNCell ( input_layer , previous_state , state_size , n_units , dtype = tf . float32 , w_init = glorot_uniform_init (), u_init = glorot_uniform_init (), bias_init = zeros_init (), activation = tf . tanh , w_dropconnect = None , u_dropconnect = None , x_dropout = None , r_dropout = None , y_dropout = None , dropout_locked = True , regularized = False , share_state_with = None , name = 'recurrent_cell' ) Args previous_state : the recurrent input Layer for the cell state_size : list of number of units for each element in the state, default is a single state with [n_units] n_units ( int ) : number of activation units for the RNN cell dtype : Layer (output) dtype input_layer the input Layer for this cell Methods: .compute_shape source . compute_shape () .reuse_with source . reuse_with ( input_layer , * previous_state , regularized = None , name = None , ** kwargs )","title":"RNNBaseCell"},{"location":"api/layers/rnn/BaseRNNCell/#_1","text":"","title":""},{"location":"api/layers/rnn/BaseRNNCell/#basernncell","text":"source BaseRNNCell ( input_layer , previous_state , state_size , n_units , dtype = tf . float32 , w_init = glorot_uniform_init (), u_init = glorot_uniform_init (), bias_init = zeros_init (), activation = tf . tanh , w_dropconnect = None , u_dropconnect = None , x_dropout = None , r_dropout = None , y_dropout = None , dropout_locked = True , regularized = False , share_state_with = None , name = 'recurrent_cell' ) Args previous_state : the recurrent input Layer for the cell state_size : list of number of units for each element in the state, default is a single state with [n_units] n_units ( int ) : number of activation units for the RNN cell dtype : Layer (output) dtype input_layer the input Layer for this cell Methods:","title":"BaseRNNCell"},{"location":"api/layers/rnn/BaseRNNCell/#compute_shape","text":"source . compute_shape ()","title":".compute_shape"},{"location":"api/layers/rnn/BaseRNNCell/#reuse_with","text":"source . reuse_with ( input_layer , * previous_state , regularized = None , name = None , ** kwargs )","title":".reuse_with"},{"location":"api/layers/rnn/GRUCell/","text":"GRUCell source GRUCell ( input_layer , n_units , previous_state = None , activation = tf . tanh , gate_activation = tf . sigmoid , w_init = tf . initializers . glorot_uniform (), u_init = tf . initializers . orthogonal (), bias_init = tf . initializers . zeros (), u_dropconnect = None , w_dropconnect = None , x_dropout = None , r_dropout = None , y_dropout = None , dropout_locked = True , regularized = False , share_state_with = None , name = 'gru_cell' ) Gated Recurrent Unit Cell. Performs a single step with a gated recurrent unit where. These units have two gates: The first defines how much do we use the values from the recurrent connection to predict the current state The second Methods: .init_state source . init_state () .compute source . compute ( input_layer , * previous_state ) .reuse_with source . reuse_with ( input_layer , * previous_state , regularized = None , name = None )","title":"GRUCell"},{"location":"api/layers/rnn/GRUCell/#_1","text":"","title":""},{"location":"api/layers/rnn/GRUCell/#grucell","text":"source GRUCell ( input_layer , n_units , previous_state = None , activation = tf . tanh , gate_activation = tf . sigmoid , w_init = tf . initializers . glorot_uniform (), u_init = tf . initializers . orthogonal (), bias_init = tf . initializers . zeros (), u_dropconnect = None , w_dropconnect = None , x_dropout = None , r_dropout = None , y_dropout = None , dropout_locked = True , regularized = False , share_state_with = None , name = 'gru_cell' ) Gated Recurrent Unit Cell. Performs a single step with a gated recurrent unit where. These units have two gates: The first defines how much do we use the values from the recurrent connection to predict the current state The second Methods:","title":"GRUCell"},{"location":"api/layers/rnn/GRUCell/#init_state","text":"source . init_state ()","title":".init_state"},{"location":"api/layers/rnn/GRUCell/#compute","text":"source . compute ( input_layer , * previous_state )","title":".compute"},{"location":"api/layers/rnn/GRUCell/#reuse_with","text":"source . reuse_with ( input_layer , * previous_state , regularized = None , name = None )","title":".reuse_with"},{"location":"api/layers/rnn/LSTMCell/","text":"LSTMCell source LSTMCell ( input_layer , n_units , previous_state = None , activation = tf . tanh , gate_activation = tf . sigmoid , bias_init = tf . initializers . zeros (), forget_bias_init = tf . initializers . ones (), w_init = tf . initializers . glorot_uniform (), u_init = tf . initializers . glorot_uniform (), w_dropconnect = None , u_dropconnect = None , x_dropout = None , r_dropout = None , y_dropout = None , dropout_locked = True , regularized = False , share_state_with = None , name = 'lstm_cell' ) A long short-term memory (LSTM) cell. Args input_layer ( Layer ) : input sequence layer in time-major order previous_state ( Optional[Tuple[Layer]] ) : (prev_h, prev_mem) previous_memory is the memory state output for the previous cell or None if the current cell is the first step Methods: .init_state source . init_state () .compute source . compute ( input_tensor , * previous_state ) compute layer value based on input Tensor values Args input_tensor : a Tensor or Layer input to the current cell previous_state : (previous_h, previous_memory) Returns a tensor with the cell's output .reuse_with source . reuse_with ( input_layer , * previous_state , regularized = None , name = None )","title":"LSTMCell"},{"location":"api/layers/rnn/LSTMCell/#_1","text":"","title":""},{"location":"api/layers/rnn/LSTMCell/#lstmcell","text":"source LSTMCell ( input_layer , n_units , previous_state = None , activation = tf . tanh , gate_activation = tf . sigmoid , bias_init = tf . initializers . zeros (), forget_bias_init = tf . initializers . ones (), w_init = tf . initializers . glorot_uniform (), u_init = tf . initializers . glorot_uniform (), w_dropconnect = None , u_dropconnect = None , x_dropout = None , r_dropout = None , y_dropout = None , dropout_locked = True , regularized = False , share_state_with = None , name = 'lstm_cell' ) A long short-term memory (LSTM) cell. Args input_layer ( Layer ) : input sequence layer in time-major order previous_state ( Optional[Tuple[Layer]] ) : (prev_h, prev_mem) previous_memory is the memory state output for the previous cell or None if the current cell is the first step Methods:","title":"LSTMCell"},{"location":"api/layers/rnn/LSTMCell/#init_state","text":"source . init_state ()","title":".init_state"},{"location":"api/layers/rnn/LSTMCell/#compute","text":"source . compute ( input_tensor , * previous_state ) compute layer value based on input Tensor values Args input_tensor : a Tensor or Layer input to the current cell previous_state : (previous_h, previous_memory) Returns a tensor with the cell's output","title":".compute"},{"location":"api/layers/rnn/LSTMCell/#reuse_with","text":"source . reuse_with ( input_layer , * previous_state , regularized = None , name = None )","title":".reuse_with"},{"location":"api/layers/rnn/RNN/","text":"RNN source RNN ( input_seq , previous_state = None , cell_config : Callable [[ Union [ Layer , tf . Tensor ]], BaseRNNCell ] = None , n_units = None , reverse = False , regularized = False , stateful = False , return_state = False , name = 'rnn_layer' , share_state_with : Optional [ 'RNN' ] = None ) Recurrent Layer Takes a batch of sequences in time-major order [time_step,batch_size,feature_size] and dynamically unrolls a RecurrentCell applying it to each time step. The sequence should have at least one time step for which the recurrent cell is first created. After that, it supports an Unknown number of time steps. (time_step>=1) Args input_seq : a Layer whose tensor has the shape [time_step,batch_size,feature_size] with time_step>=1 Attributes cell : a Layer of type RecurrentCell used in the unrolled steps cell_config ( Callable[Layer]) : a function returning a recurrent cell Layer` when applied to an input or tensor. This can be solved by creating a lambda with the sell parameters or a partial Methods: .compute_shape source . compute_shape () .init_state source . init_state () Create a recurrent cell from the given config Dev note The only stateful thing here is the cell which is a layer. Since layers Need to know their input layer for their state to be initialized, we need to give the cell a dummy input. Returns state ( LayerState ) : a state with a cell layer that performs the computations .compute source . compute ( input_seq , * prev_state ) .reuse_with source . reuse_with ( input_seq , * previous_state , regularized = None , reverse = None , stateful = None , return_state = None , name = None ) .reset source . reset ()","title":"RNN"},{"location":"api/layers/rnn/RNN/#_1","text":"","title":""},{"location":"api/layers/rnn/RNN/#rnn","text":"source RNN ( input_seq , previous_state = None , cell_config : Callable [[ Union [ Layer , tf . Tensor ]], BaseRNNCell ] = None , n_units = None , reverse = False , regularized = False , stateful = False , return_state = False , name = 'rnn_layer' , share_state_with : Optional [ 'RNN' ] = None ) Recurrent Layer Takes a batch of sequences in time-major order [time_step,batch_size,feature_size] and dynamically unrolls a RecurrentCell applying it to each time step. The sequence should have at least one time step for which the recurrent cell is first created. After that, it supports an Unknown number of time steps. (time_step>=1) Args input_seq : a Layer whose tensor has the shape [time_step,batch_size,feature_size] with time_step>=1 Attributes cell : a Layer of type RecurrentCell used in the unrolled steps cell_config ( Callable[Layer]) : a function returning a recurrent cell Layer` when applied to an input or tensor. This can be solved by creating a lambda with the sell parameters or a partial Methods:","title":"RNN"},{"location":"api/layers/rnn/RNN/#compute_shape","text":"source . compute_shape ()","title":".compute_shape"},{"location":"api/layers/rnn/RNN/#init_state","text":"source . init_state () Create a recurrent cell from the given config Dev note The only stateful thing here is the cell which is a layer. Since layers Need to know their input layer for their state to be initialized, we need to give the cell a dummy input. Returns state ( LayerState ) : a state with a cell layer that performs the computations","title":".init_state"},{"location":"api/layers/rnn/RNN/#compute","text":"source . compute ( input_seq , * prev_state )","title":".compute"},{"location":"api/layers/rnn/RNN/#reuse_with","text":"source . reuse_with ( input_seq , * previous_state , regularized = None , reverse = None , stateful = None , return_state = None , name = None )","title":".reuse_with"},{"location":"api/layers/rnn/RNN/#reset","text":"source . reset ()","title":".reset"},{"location":"api/layers/rnn/RNNCell/","text":"RNNCell source RNNCell ( input_layer , n_units , previous_state = None , activation = tf . tanh , w_init = tf . initializers . glorot_uniform (), u_init = tf . initializers . glorot_uniform (), bias_init = tf . initializers . zeros (), share_state_with = None , w_dropconnect = None , u_dropconnect = None , r_dropout = None , x_dropout = None , y_dropout = None , dropout_locked = True , regularized = False , name = 'rnn_cell' ) Recurrent Cell Corresponds to a single step on an unrolled RNN network Args input_layer : the input layer to the RNN Cell n_units : number of output units for this RNN Cell previous_state : a RNNCell from which we can extract output activation : activation function to be used in the cell share_state_with : a Layer with the same number of units than this Cell name : name for the RNN cell share_state_with ( RNNCell or None ): Methods: .init_state source . init_state () .compute source . compute ( input_layer , * previous_state )","title":"RNNCell"},{"location":"api/layers/rnn/RNNCell/#_1","text":"","title":""},{"location":"api/layers/rnn/RNNCell/#rnncell","text":"source RNNCell ( input_layer , n_units , previous_state = None , activation = tf . tanh , w_init = tf . initializers . glorot_uniform (), u_init = tf . initializers . glorot_uniform (), bias_init = tf . initializers . zeros (), share_state_with = None , w_dropconnect = None , u_dropconnect = None , r_dropout = None , x_dropout = None , y_dropout = None , dropout_locked = True , regularized = False , name = 'rnn_cell' ) Recurrent Cell Corresponds to a single step on an unrolled RNN network Args input_layer : the input layer to the RNN Cell n_units : number of output units for this RNN Cell previous_state : a RNNCell from which we can extract output activation : activation function to be used in the cell share_state_with : a Layer with the same number of units than this Cell name : name for the RNN cell share_state_with ( RNNCell or None ): Methods:","title":"RNNCell"},{"location":"api/layers/rnn/RNNCell/#init_state","text":"source . init_state ()","title":".init_state"},{"location":"api/layers/rnn/RNNCell/#compute","text":"source . compute ( input_layer , * previous_state )","title":".compute"},{"location":"api/layers/utils/LayerConfig/","text":"LayerConfig source LayerConfig ( layer_cls , ** kwargs ) LayerConfig A Layer configuration is a Callable that captures all the arguments of a layer construction except input layers. This allows us to delay calling the constructor a Layer, thus delaying the creation of it's state. LayerConfig object also validate the constructor arguments of its target Layer type. Note Layer subtypes have a class method config() you can use as an alternative to importing LayerConfig as: python import tensorx as tx config = tx.Linear.config(n_units=3) Layer instances have a config field that returns a configuration with the current object configuration python import tensorx as tx y = tx.Linear(tf.ones([2,2])) config = y.config assert \"n_units\" in config.arg_dict Attributes layer_cls ( Callable[Layer] ) : the current Layer subtype for this configuration arg_spec ( inspect.FullArgSpec ) : argspec (args, var args, defaults, etc) of the constructor of the target class arg_names ( Set[str] ) : a set of name for the constructor arguments arg_dict ( Dict[str,Any] ) : dictionary with current argument values for the configuration Args layer_cls ( Callable[Layer] ) : a Layer subtype for which we're trying to build a configuration kwargs ( Dict[str,Any] ) : a dict mapping arg names to values Methods: .filter_args source . filter_args ( ** kwargs ) filter_args filters a given keyword argument dictionary removing any argument that is not present in the constructor for the current Layer type. Args kwargs ( Dict['str',Any] ) : keyword arguments to be filtered Returns new_kwargs ( Dict['str',Any] ) : new filtered kwargs .update source . update ( ** kwargs ) update Updates the config constructor argument dictionary and validates those parameters. Args kwargs ( Dict['str',Any] ) : new values for constructor named arguments to be updated","title":"LayerConfig"},{"location":"api/layers/utils/LayerConfig/#_1","text":"","title":""},{"location":"api/layers/utils/LayerConfig/#layerconfig","text":"source LayerConfig ( layer_cls , ** kwargs ) LayerConfig A Layer configuration is a Callable that captures all the arguments of a layer construction except input layers. This allows us to delay calling the constructor a Layer, thus delaying the creation of it's state. LayerConfig object also validate the constructor arguments of its target Layer type. Note Layer subtypes have a class method config() you can use as an alternative to importing LayerConfig as: python import tensorx as tx config = tx.Linear.config(n_units=3) Layer instances have a config field that returns a configuration with the current object configuration python import tensorx as tx y = tx.Linear(tf.ones([2,2])) config = y.config assert \"n_units\" in config.arg_dict Attributes layer_cls ( Callable[Layer] ) : the current Layer subtype for this configuration arg_spec ( inspect.FullArgSpec ) : argspec (args, var args, defaults, etc) of the constructor of the target class arg_names ( Set[str] ) : a set of name for the constructor arguments arg_dict ( Dict[str,Any] ) : dictionary with current argument values for the configuration Args layer_cls ( Callable[Layer] ) : a Layer subtype for which we're trying to build a configuration kwargs ( Dict[str,Any] ) : a dict mapping arg names to values Methods:","title":"LayerConfig"},{"location":"api/layers/utils/LayerConfig/#filter_args","text":"source . filter_args ( ** kwargs ) filter_args filters a given keyword argument dictionary removing any argument that is not present in the constructor for the current Layer type. Args kwargs ( Dict['str',Any] ) : keyword arguments to be filtered Returns new_kwargs ( Dict['str',Any] ) : new filtered kwargs","title":".filter_args"},{"location":"api/layers/utils/LayerConfig/#update","text":"source . update ( ** kwargs ) update Updates the config constructor argument dictionary and validates those parameters. Args kwargs ( Dict['str',Any] ) : new values for constructor named arguments to be updated","title":".update"},{"location":"api/layers/utils/LayerState/","text":"LayerState source LayerState () LayerState A LayerState is a container to store tf.Variable , Layer , or other tensors to be shared between layers. Methods: .variables source . variables () variables returns a list of all variables in the layer state Returns variables ( List[tf.Variable] ) : a list of all variables in the layer state .var_dict source . var_dict () var_dict gets all variables in the layer state as a dict from a hashable reference ( variable.ref() ) object to tf.Variable . Returns var_dict ( Dict[Hashable,Variable] ) : a dictionary with all the variables in the current layer state.","title":"LayerState"},{"location":"api/layers/utils/LayerState/#_1","text":"","title":""},{"location":"api/layers/utils/LayerState/#layerstate","text":"source LayerState () LayerState A LayerState is a container to store tf.Variable , Layer , or other tensors to be shared between layers. Methods:","title":"LayerState"},{"location":"api/layers/utils/LayerState/#variables","text":"source . variables () variables returns a list of all variables in the layer state Returns variables ( List[tf.Variable] ) : a list of all variables in the layer state","title":".variables"},{"location":"api/layers/utils/LayerState/#var_dict","text":"source . var_dict () var_dict gets all variables in the layer state as a dict from a hashable reference ( variable.ref() ) object to tf.Variable . Returns var_dict ( Dict[Hashable,Variable] ) : a dictionary with all the variables in the current layer state.","title":".var_dict"},{"location":"api/train/Callback/","text":"Callback source Callback ( trigger_dict , priority = 1 , properties = None ) General Purpose Callback Callback objects can be registered at a scheduler to be executed on specific triggers they are also sortable by priority Args trigger_dict : a dictionary mapping triggers to functions to be executed properties (List[str]) : a list of properties required by this callback priority : int value which dictates callback execution priority (lower values take priority)","title":"Callback"},{"location":"api/train/Callback/#_1","text":"","title":""},{"location":"api/train/Callback/#callback","text":"source Callback ( trigger_dict , priority = 1 , properties = None ) General Purpose Callback Callback objects can be registered at a scheduler to be executed on specific triggers they are also sortable by priority Args trigger_dict : a dictionary mapping triggers to functions to be executed properties (List[str]) : a list of properties required by this callback priority : int value which dictates callback execution priority (lower values take priority)","title":"Callback"},{"location":"api/train/Model/","text":"Model source Model ( run_outputs , run_inputs = None , train_outputs = None , train_inputs = None , train_loss = None , eval_inputs = None , eval_outputs = None , eval_score = None , name = 'Model' ) Base Model Args train_loss : either a callable, layer, or dictionary mapping a callable or train_inputs : defaults to run inputs, if loss is provided you can either layer to the target outputs. supply the inputs to the train graph that include the loss, or let the Model create inputs for you. Methods: .draw source . draw ( path = 'graph.pdf' ) .set_optimizer source . set_optimizer ( optimizer , ** config ) Set the optimizer for this model Optimizer Hyper-Parameters The arguments passed to the optimizer constructor can be either regular Python values, tensors, or a callable . If they are callable, they will called during apply_gradients() to get the value for the hyper parameter. Args optimizer (Optimizer) : optimizer class or instance config : dictionary with parameters for the optimizer, if you want to modify these parameters during training pass an tx.Param as the value for the given parameter instead of constant value. Returns optimizer (Optimizer) : the configured optimizer instance. .run source . run ( input_feed , compiled_graph = False ) .train_step source . train_step ( input_feed ) .eval_step source . eval_step ( input_feed ) Args input_feed: Returns *eval_output, eval_score ((eval outputs,eval score)): .train source . train ( train_data , validation_data = None , test_data = None , epochs = 1 , steps_per_epoch = None , callbacks = [] ) Main training loop Args train_data : an iterable of dictionaries from Input Layers to values {Input:data}. validation_data : an iterable of dictionaries from Input Layers to values {Input:data}. test_data : an iterable of dictionaries from Input Layers to values {Input:data}. epochs (int) : number of training epochs. steps_per_epoch : number of steps in an epoch, if not None, epochs are incremented each time callbacks : Callback functions scheduled during the training. (calling iter on this object should yield an iterator for an epoch.) this number of steps pass even if the entire train_data has not been transversed.","title":"Model"},{"location":"api/train/Model/#_1","text":"","title":""},{"location":"api/train/Model/#model","text":"source Model ( run_outputs , run_inputs = None , train_outputs = None , train_inputs = None , train_loss = None , eval_inputs = None , eval_outputs = None , eval_score = None , name = 'Model' ) Base Model Args train_loss : either a callable, layer, or dictionary mapping a callable or train_inputs : defaults to run inputs, if loss is provided you can either layer to the target outputs. supply the inputs to the train graph that include the loss, or let the Model create inputs for you. Methods:","title":"Model"},{"location":"api/train/Model/#draw","text":"source . draw ( path = 'graph.pdf' )","title":".draw"},{"location":"api/train/Model/#set_optimizer","text":"source . set_optimizer ( optimizer , ** config ) Set the optimizer for this model Optimizer Hyper-Parameters The arguments passed to the optimizer constructor can be either regular Python values, tensors, or a callable . If they are callable, they will called during apply_gradients() to get the value for the hyper parameter. Args optimizer (Optimizer) : optimizer class or instance config : dictionary with parameters for the optimizer, if you want to modify these parameters during training pass an tx.Param as the value for the given parameter instead of constant value. Returns optimizer (Optimizer) : the configured optimizer instance.","title":".set_optimizer"},{"location":"api/train/Model/#run","text":"source . run ( input_feed , compiled_graph = False )","title":".run"},{"location":"api/train/Model/#train_step","text":"source . train_step ( input_feed )","title":".train_step"},{"location":"api/train/Model/#eval_step","text":"source . eval_step ( input_feed ) Args input_feed: Returns *eval_output, eval_score ((eval outputs,eval score)):","title":".eval_step"},{"location":"api/train/Model/#train","text":"source . train ( train_data , validation_data = None , test_data = None , epochs = 1 , steps_per_epoch = None , callbacks = [] ) Main training loop Args train_data : an iterable of dictionaries from Input Layers to values {Input:data}. validation_data : an iterable of dictionaries from Input Layers to values {Input:data}. test_data : an iterable of dictionaries from Input Layers to values {Input:data}. epochs (int) : number of training epochs. steps_per_epoch : number of steps in an epoch, if not None, epochs are incremented each time callbacks : Callback functions scheduled during the training. (calling iter on this object should yield an iterator for an epoch.) this number of steps pass even if the entire train_data has not been transversed.","title":".train"},{"location":"api/train/model/","text":"{{autogenerated}}","title":"Model"},{"location":"api/utils/Graph/","text":"Graph source Graph () Graph Simple append-only graph data structure. It keeps track of nodes, directed edges, and endpoint nodes. Note A node without edges counts as both an input and output node of the graph Attributes nodes (set) : set of node objects (object need tobe hashable) edges_in (dict) : a dictionary that maps nodes in the keys to a list of notes with edges coming into that node edges_out (dict) : a dictionary that maps nodes in the keys to a list of notes with edges coming from that node in_nodes (dict) : key-only dictionary (ordered set) with input nodes (nodes without input edges). out_nodes (dict) : key-only dictionary (ordered set) with output nodes of the graph (nodes without output edges) Methods: .add_node source . add_node ( node ) .add_edge source . add_edge ( node1 , node2 ) Adds a new edge to the graph also removes nodes from input roots or outputs to reflect the current edge if necessary. Args node1 ( Node ) : starting node node2 ( Node ) : ending node .dependency_iter source . dependency_iter () returns a dictionary with a map from nodes to dependency priorities with lower values having higher priority. Keys are ordered by priority from lower to higher and number of dependencies from lower to higher Notes: Transversing a graph by priority guarantees that when we visit a node all it's dependencies have already been visited, additionally, ordering by number of dependencies guarantees that we can maintain a minimum result cache when transversing the graph. Returns nodes ( dict ) : dictionary from nodes to (priorities,number of dependencies) .as_function source . as_function ( ord_inputs = None , ord_outputs = None , name = 'compiled_graph' , compile = True ) compiles the graph into a tensorflow callable compiled graph Converts the current graph into a function with a series of layer.compute(*tensors) calls and uses tf.function to compile this function to a Tensorflow static graph if compile is True . The resulting function is a closure with access to layer objects, to TensorFlow should be able to trace the computations for each layer compute call. Another way to feed inputs to a graph is to use input layers and change the value, if the graphs are created without inputs, but the terminal input nodes are Dynamic Inputs, the execution of those layers is a read on their placeholder value, which you can change that value before calling the graph and the output will be correct. input_layer . value = in0 input_Layer . value = in1 outputs = graph () this adds a bit of a overhead since we have to write to the variable Dev Note makes use of dependency_iter to create the computation calls such that when we call compute all the inputs needed as dependencies are already available. Args ord_inputs ( List[Node] ) : list of input that determines the order of resulting function arguments ord_outputs ( List[Node ]) : list of outputs used to determine the return order name ( str ) : function name, must be a valid python function name compile ( bool ) : if True, returns a tensorflow graph else returns a python function Returns function ( Callable ) : an optimized TensorFlow static graph as a callable function or a python function .as_function_v2 source . as_function_v2 ( ord_inputs = None , ord_outputs = None , fn_name = 'compiled_graph' , stateful_inputs = False , compile = True ) .draw source . draw ( path ) .compute source . compute ( * input_values ) computes the graph output values based on the given input values Args input_values : input values with the same order as the graph inputs, or a dictionary mapping values to input layers. Returns a tuple with the values for the correspondent graph outputs","title":"Graph"},{"location":"api/utils/Graph/#_1","text":"","title":""},{"location":"api/utils/Graph/#graph","text":"source Graph () Graph Simple append-only graph data structure. It keeps track of nodes, directed edges, and endpoint nodes. Note A node without edges counts as both an input and output node of the graph Attributes nodes (set) : set of node objects (object need tobe hashable) edges_in (dict) : a dictionary that maps nodes in the keys to a list of notes with edges coming into that node edges_out (dict) : a dictionary that maps nodes in the keys to a list of notes with edges coming from that node in_nodes (dict) : key-only dictionary (ordered set) with input nodes (nodes without input edges). out_nodes (dict) : key-only dictionary (ordered set) with output nodes of the graph (nodes without output edges) Methods:","title":"Graph"},{"location":"api/utils/Graph/#add_node","text":"source . add_node ( node )","title":".add_node"},{"location":"api/utils/Graph/#add_edge","text":"source . add_edge ( node1 , node2 ) Adds a new edge to the graph also removes nodes from input roots or outputs to reflect the current edge if necessary. Args node1 ( Node ) : starting node node2 ( Node ) : ending node","title":".add_edge"},{"location":"api/utils/Graph/#dependency_iter","text":"source . dependency_iter () returns a dictionary with a map from nodes to dependency priorities with lower values having higher priority. Keys are ordered by priority from lower to higher and number of dependencies from lower to higher Notes: Transversing a graph by priority guarantees that when we visit a node all it's dependencies have already been visited, additionally, ordering by number of dependencies guarantees that we can maintain a minimum result cache when transversing the graph. Returns nodes ( dict ) : dictionary from nodes to (priorities,number of dependencies)","title":".dependency_iter"},{"location":"api/utils/Graph/#as_function","text":"source . as_function ( ord_inputs = None , ord_outputs = None , name = 'compiled_graph' , compile = True ) compiles the graph into a tensorflow callable compiled graph Converts the current graph into a function with a series of layer.compute(*tensors) calls and uses tf.function to compile this function to a Tensorflow static graph if compile is True . The resulting function is a closure with access to layer objects, to TensorFlow should be able to trace the computations for each layer compute call. Another way to feed inputs to a graph is to use input layers and change the value, if the graphs are created without inputs, but the terminal input nodes are Dynamic Inputs, the execution of those layers is a read on their placeholder value, which you can change that value before calling the graph and the output will be correct. input_layer . value = in0 input_Layer . value = in1 outputs = graph () this adds a bit of a overhead since we have to write to the variable Dev Note makes use of dependency_iter to create the computation calls such that when we call compute all the inputs needed as dependencies are already available. Args ord_inputs ( List[Node] ) : list of input that determines the order of resulting function arguments ord_outputs ( List[Node ]) : list of outputs used to determine the return order name ( str ) : function name, must be a valid python function name compile ( bool ) : if True, returns a tensorflow graph else returns a python function Returns function ( Callable ) : an optimized TensorFlow static graph as a callable function or a python function","title":".as_function"},{"location":"api/utils/Graph/#as_function_v2","text":"source . as_function_v2 ( ord_inputs = None , ord_outputs = None , fn_name = 'compiled_graph' , stateful_inputs = False , compile = True )","title":".as_function_v2"},{"location":"api/utils/Graph/#draw","text":"source . draw ( path )","title":".draw"},{"location":"api/utils/Graph/#compute","text":"source . compute ( * input_values ) computes the graph output values based on the given input values Args input_values : input values with the same order as the graph inputs, or a dictionary mapping values to input layers. Returns a tuple with the values for the correspondent graph outputs","title":".compute"},{"location":"start/install/","text":"Installation TensorX is written in pure python but it's made to complement Tensorflow , so it depends on Tensorflow , which needs to be installed from the tensorflow package. The reason for this is that you might want to install Tensorflow builds optimized for your machine (see these ). Additionally, TensorX has optional dependencies like matplotlib or pygraphviz for certain functionality. with pip Install using pip with the following commands pip install tensorflow pip install tensorx You can install a custom tensorflow wheel using it's URL like so: pip install https://github.com/davidenunes/tensorflow-wheels/releases/download/r2.3.cp38.gpu/tensorflow-2.3.0-cp38-cp38-linux_x86_64.whl with Poetry TensorX is easy to install using Poetry which manages the dependencies and a virtualenv for you. See Poetry documentation for details on installation and usage. Once Poetry is installed, you can use it to install TensorX either from PyPI , or directly from git . If you want to create a new project using poetry, simply run: poetry new myproject cd myproject if you already have a project, move to it's directory and run the following commands: poetry add tensorflow poetry add tensorx # or, if you want to have tensorx as a dependency from the git repository poetry add git+https://github.com/davidenunes/tensorx.git Developer Installation For a developer installation of TensorX, simply clone the git repository and install it with Poetry. The git repository has a pyproject.toml and poetry.lock files. These allow for the installation to be reproducible --meaning that Poetry will install the exact versions of dependencies that were being used on a specific commit. # Clone TensorX git repository git clone https://github.com/davidenunes/tensorx.git cd tensorx # Install dependencies poetry install # You need Tensorflow which is not specified as a dependency # to install it without adding it as a dependency simple run poetry run pip install tensorflow poetry run will run a given command inside the project current virtualenv","title":"Installation"},{"location":"start/install/#installation","text":"TensorX is written in pure python but it's made to complement Tensorflow , so it depends on Tensorflow , which needs to be installed from the tensorflow package. The reason for this is that you might want to install Tensorflow builds optimized for your machine (see these ). Additionally, TensorX has optional dependencies like matplotlib or pygraphviz for certain functionality.","title":"Installation"},{"location":"start/install/#with-pip","text":"Install using pip with the following commands pip install tensorflow pip install tensorx You can install a custom tensorflow wheel using it's URL like so: pip install https://github.com/davidenunes/tensorflow-wheels/releases/download/r2.3.cp38.gpu/tensorflow-2.3.0-cp38-cp38-linux_x86_64.whl","title":"with pip"},{"location":"start/install/#with-poetry","text":"TensorX is easy to install using Poetry which manages the dependencies and a virtualenv for you. See Poetry documentation for details on installation and usage. Once Poetry is installed, you can use it to install TensorX either from PyPI , or directly from git . If you want to create a new project using poetry, simply run: poetry new myproject cd myproject if you already have a project, move to it's directory and run the following commands: poetry add tensorflow poetry add tensorx # or, if you want to have tensorx as a dependency from the git repository poetry add git+https://github.com/davidenunes/tensorx.git","title":"with Poetry"},{"location":"start/install/#developer-installation","text":"For a developer installation of TensorX, simply clone the git repository and install it with Poetry. The git repository has a pyproject.toml and poetry.lock files. These allow for the installation to be reproducible --meaning that Poetry will install the exact versions of dependencies that were being used on a specific commit. # Clone TensorX git repository git clone https://github.com/davidenunes/tensorx.git cd tensorx # Install dependencies poetry install # You need Tensorflow which is not specified as a dependency # to install it without adding it as a dependency simple run poetry run pip install tensorflow poetry run will run a given command inside the project current virtualenv","title":"Developer Installation"},{"location":"start/tutorial/","text":"Tutorial This tutorial introduces basic TensorX concepts. Prerequisites TensorX is a machine learning library to build neural network models written in Python and it works as a complement to Tensorflow , therefore, to make the most out of this tutorial (and this library), readers should be familiarized with the following: Python 3 : if you're new to the Python language or need to refresh some concepts check out the Python Tutorial . Tensorflow : Tensorflow is a high-performance machine learning library that allows numerical computation on GPUs and TPUs . It was originally designed to build auto-differentiable dataflow graphs. Although it has adopted an eager execution model in version 2, computation graphs are still an integral concept of high performance tensorflow program definition and deployment. Unfortunately most tensorflow tutorials and guides, focus on Keras, a high level interface similar to TensorX. For a primer on Tensorflow, I recommend taking a look at Tensorflow basics guide section instead. NumPy : similarly to the core of Tensorflow, Numpy is a numerical computation library with a focus in multi-dimensional array transformations. Given that Tensorflow tensors are converted to and from NumPy arrays, and TensorX also depends on NumPy (mostly for testing), it is recommended that the reader is familiarized with NumPy basics . For more details, check the NumPy documentation . Installation You can install tensorx with pip as follows: pip install tensorflow pip install tensorx for more details see the installation documentation . Layers In TensorX, a Layer is the basic building block of a neural network . Semantically speaking, a layer is an object that can have multiple inputs, an inner state, and a computation function applied to its inputs (that depends on the current state). Each layer has a single output. In essence,we can say that a Layer instance is a stateful function. Connecting a series of layers results in a layer graph . In TensorX, each layer is the end-node of a subgraph, and executing it will result in the execution of all layers in the subgraph with the current layer as output. Layer subclasses can range from simple linear transformations (e.g. Layer ) to more complex layers used to build recurrent neural networks such as long short-term memory (LSTM) cells (e.g. LSTMCell ) or attention mechanisms such as MHAttention . Layer properties and methods inputs : list of input layers for the current layer; input : syntax sugar for inputs[0] ; n_units : number of output units or neurons, this is the last dimension of the output tensor resulting from this layer's computation; shape : the inferred shape for the layer output; compute(*tensors) : layer computation applied to its input layers or input tensors if any is given. __call__ : all layers are Callable and the result is the computation of the entire layer graph taking the current layer as the terminal node. reuse_with : create a new layer object that shares the state with the current layer but is connected to different inputs. The new layer is the end-point node of a new layer graph. variables : a list of tf.Variable objects that handled by the current layer trainable_variables : a list of tf.Variable objects that are trainable , this is, that are changed by an optimizer during training.* config : a layer configuration ( LayerConfig ) with the arguments used in the current layer instance constructor. Using existing Layers TensorX ships with a number of built in Layers that you can easily use to compose layer graphs that perform various computations. All layers are accessible from the global namespace tensorx.Linear or from the tensorx.layers module. The following example shows how to use a simple Linear layer that performs the computation y=Wx+b y=Wx+b : import tensorflow as tf import tensorx as tx x = tf . random . uniform ([ 2 , 2 ], dtype = tf . float32 ) # y = Wx + b y = tx . Linear ( x , n_units = 3 ) result = y () assert tx . tensor_equal ( tf . shape ( result ), [ 2 , 3 ]) assert len ( y . inputs ) == 1 assert isinstance ( y . input , tx . Constant ) Note that we can pass a Tensor object to Linear (or any other layer), and it will be automatically converted to a Layer , to a Constant layer to be more precise. The layer y has exactly 1 input layer and __call__ will return the result of its computation on this input. Dynamic stateful Input The Input layer allows us to add a dynamic input to a layer graph: value = tf . random . uniform ([ 2 , 2 ], dtype = tf . float32 ) x = tx . Input ( init_value = value ) # y = Wx + b y = tx . Linear ( x , n_units = 3 ) result1 = y () # x is stateful and its value can be changed e.g. to a new random value x . value = tf . random . uniform ([ 2 , 2 ], dtype = tf . float32 ) result2 = y () result3 = y . compute ( value ) assert not tx . tensor_equal ( result1 , result2 ) assert not y . input . constant # compute returns the layer computation independently from its current graph assert tx . tensor_equal ( result1 , result3 ) print ( result1 ) tf.Tensor ( [[ 0 .8232075 0 .2716378 -0.33215973 ] [ 0 .34996247 -0.02594224 -0.05033442 ]] , shape =( 2 , 3 ) , dtype = float32 ) Input allows the creation of dynamic input layers with a value property that can be changed, we can see that the value at the end-point of this graph changes as well. Moreover, the compute method is distinct from __call__ as it only depends on the layer current state and not on the current graph. Important if n_units is not set to None on a dynamic Input layer, it will take the last dimension of the initial value, henceforth, any tensor assigned to value must match the n_units in its last dimension. This means that the batch dimension can be variable for example. Warning You can't switch the number of dimension in a dynamic Input . Without an initial value or shape, it defaults to a shape (0, 0) (an empty tensor with 2 dimensions). An error is thrown if you try to assign a tensor with a mismatching number of dimensions. For example, if you create an input as follows Input(shape=[None,None,None]) , an error is thrown if you assign a tensor with a mismatching number of dimensions like input.value = tf.ones([2,2]) . Re-Using Layers When you create a new Layer object, usually you will pass it its input layers which will then make it the end-node of a graph connected to those input layers. This will also call the init_state method which initializes any tf.Variable objects that might be part of the layers' state. If you want to re-use this layer with a different set of input layers, you can use the reuse_with method. This creates a new layer with all the same parameters, additionally this new layer will share it's state with the previous one. import tensorflow as tf import tensorx as tx # stateful input placeholder x1 = tx . Input ( n_units = 2 ) x1 . value = tf . random . uniform ([ 2 , 2 ]) #y = Wx + b l1 = tx . Linear ( x1 , n_units = 3 ) a1 = tx . Activation ( l1 , tx . relu ) l2 = tx . Linear ( a1 , n_units = 4 ) d1 = tx . Dropout ( a1 , probability = 0.4 ) l3 = l2 . reuse_with ( d1 ) Warning Any changes to the state of one layer will affect the state of the second. Re-Using Modules A Module is a special layer which creates a single Layer from a given layer graph . A layer graph is a set of layers connected to each other. For example: x = tx . Input ( tf . ones ([ 2 , 2 ])) y1 = tx . Linear ( x , 3 ) y2 = tx . Linear ( y1 , 4 ) m = tx . Module ( inputs = x , output = y2 ) assert tx . tensor_equal ( m (), y2 ()) You can take the two Linear layers and create a single module with a state shared with both layers. Like with any other layer you can also call reuse_with on a module and in this case, the entire state of the two Linear layers will again be shared with the newly created Module . Gradients and Autodiff Automatic differentiation is a cornerstone of most deep learning frameworks. TensorFlow remembers what operations happen and in what order during the forward pass, then, during the backpropagation pass, TensorFlow traverses this list of operations in reverse order to compute gradients --usually with respect to some input like a tf.Variable . Automatic differentiation can be accessed in Tensorflow using the tf.GradientTape context. Whatever is executed inside the GradientTape context, gets tracked so that the gradients with respect to some variables can be computed: import tensorflow as tf x = tf . Variable ( 3.0 ) with tf . GradientTape () as tape : y = x ** 2 # dy = 2x * dx dy_dx = tape . gradient ( y , x ) TensorX Layers describe operations over tensors in terms of tensorflow operations, and store their state in tf.Variable objects, so layers executed inside the tf.GradientTape context are tracked just like any other Tensorflow operation. With this in mind, we can then compute the gradients of a particular value with respect to the trainable_variables used in the computation. For example: import tensorflow as tf import tensorx as tx x = tx . Input ( n_units = 3 ) # y = Wx + b y = tx . Linear ( x , 3 , add_bias = True ) loss = tx . Lambda ( y , fn = lambda v : tf . reduce_mean ( v ** 2 )) x . value = [[ 1. , 2. , 3. ]] with tf . GradientTape () as tape : loss_value = loss () # we could have done this as well # v = y() # loss_value = tf.reduce_mean(v ** 2) grads = tape . gradient ( loss_value , y . trainable_variables ) assert len ( y . trainable_variables ) == 2 assert len ( grads ) == 2 assert grads [ 0 ] . shape == y . weights . shape assert grads [ 1 ] . shape == y . bias . shape In this case, only the weights , and bias of the Linear layer are trainable variables, so we can take the gradient of loss_value with respect to these variables, the result is a list of tensors with the same shape as the variables used as targets. Tip In these examples we're still using an eager execution model from Tensorflow, as we will see, this is good for debugging, but not very efficient. Next in this tutorial, we will show how we can compile TensorX layer graphs into Tensorflow graphs using the tf.function . Graph Compilation TensorX builds layer graphs automatically from layer objects are connected to each other. These graphs are, in effect, directed acyclic graphs (DAG) defining a given computation over inputs. To aid with validation and execution of neural network layer graphs, TensorX has a Graph utility class. The Graph class allows for automatic graph construction from output nodes (by recursively visiting each node's inputs). It also facilitates transversal by dependency ordering along with conversion of arbitrary graphs to functions and Tensorflow static graphs. TensorX takes advantage of Tensorflow's graph optimization system to simplify and optimize Layer computations. It does this by converting layer graphs into functions that are then trace-compiled into an optimized TensorFlow static graphs. x1 = Input ( n_units = 2 ) x2 = Input ( n_units = 4 ) l1 = Linear ( x1 , 4 ) l2 = Add ( l1 , x2 ) l3 = Linear ( l2 , 2 ) g = Graph . build ( outputs = l3 , inputs = [ x1 , x2 ]) fn = g . as_function ( compile = True ) # fn is holding the following function @tf . function def compiled_graph (): x1 = layers [ \"x1\" ] . compute () x2 = layers [ \"x2\" ] . compute () l1 = layers [ \"l1\" ] . compute ( x1 ) l2 = layers [ \"l2\" ] . compute ( l1 , x2 ) l3 = layers [ \"l3\" ] . compute ( l2 ) return l3 If no ord_inputs are given to as_function , the resulting function doesn't define input parameters. To feed values to such a function we would need to change the values of the inputs with x1.value = ... before calling fn() . If ord_inputs are passed (e.g. g.as_function(ord_inputs=[x1,x2]) ), these will map the parameters to the corresponding layers that must be inputs of the current graph, if so, the resulting function can be called with arguments as fn(value1,value2) . Just as Layer objects define implicit subgraphs, we can also build Callable functions and TensorFlow static graphs from any layer by calling layer.as_function() . Much like in the previous example, doing this will return a function without parameters. This is just syntax sugar for: ... graph = Graph . build ( inputs = None , outputs = self ) return graph . as_function ( name = name , compile = compile ) Dev Notes A function conversion procedure which uses parameters with optional values for Input layers is in development. Models TensorX uses the Model class to group together multiple layer graphs and simplify the configuration of a training loop with multiple callbacks. This part of the API might suffer some changes but at its core it's just intended as a way to group together layer graphs, optimizers , and a configurable training loop with Callbacks . Docs in progress Finish this documentation with examples Callbacks Docs in progress Finish this documentation with examples Serialization Docs in progress Finish this documentation with examples","title":"Tutorial"},{"location":"start/tutorial/#tutorial","text":"This tutorial introduces basic TensorX concepts.","title":"Tutorial"},{"location":"start/tutorial/#prerequisites","text":"TensorX is a machine learning library to build neural network models written in Python and it works as a complement to Tensorflow , therefore, to make the most out of this tutorial (and this library), readers should be familiarized with the following: Python 3 : if you're new to the Python language or need to refresh some concepts check out the Python Tutorial . Tensorflow : Tensorflow is a high-performance machine learning library that allows numerical computation on GPUs and TPUs . It was originally designed to build auto-differentiable dataflow graphs. Although it has adopted an eager execution model in version 2, computation graphs are still an integral concept of high performance tensorflow program definition and deployment. Unfortunately most tensorflow tutorials and guides, focus on Keras, a high level interface similar to TensorX. For a primer on Tensorflow, I recommend taking a look at Tensorflow basics guide section instead. NumPy : similarly to the core of Tensorflow, Numpy is a numerical computation library with a focus in multi-dimensional array transformations. Given that Tensorflow tensors are converted to and from NumPy arrays, and TensorX also depends on NumPy (mostly for testing), it is recommended that the reader is familiarized with NumPy basics . For more details, check the NumPy documentation .","title":"Prerequisites"},{"location":"start/tutorial/#installation","text":"You can install tensorx with pip as follows: pip install tensorflow pip install tensorx for more details see the installation documentation .","title":"Installation"},{"location":"start/tutorial/#layers","text":"In TensorX, a Layer is the basic building block of a neural network . Semantically speaking, a layer is an object that can have multiple inputs, an inner state, and a computation function applied to its inputs (that depends on the current state). Each layer has a single output. In essence,we can say that a Layer instance is a stateful function. Connecting a series of layers results in a layer graph . In TensorX, each layer is the end-node of a subgraph, and executing it will result in the execution of all layers in the subgraph with the current layer as output. Layer subclasses can range from simple linear transformations (e.g. Layer ) to more complex layers used to build recurrent neural networks such as long short-term memory (LSTM) cells (e.g. LSTMCell ) or attention mechanisms such as MHAttention .","title":"Layers"},{"location":"start/tutorial/#layer-properties-and-methods","text":"inputs : list of input layers for the current layer; input : syntax sugar for inputs[0] ; n_units : number of output units or neurons, this is the last dimension of the output tensor resulting from this layer's computation; shape : the inferred shape for the layer output; compute(*tensors) : layer computation applied to its input layers or input tensors if any is given. __call__ : all layers are Callable and the result is the computation of the entire layer graph taking the current layer as the terminal node. reuse_with : create a new layer object that shares the state with the current layer but is connected to different inputs. The new layer is the end-point node of a new layer graph. variables : a list of tf.Variable objects that handled by the current layer trainable_variables : a list of tf.Variable objects that are trainable , this is, that are changed by an optimizer during training.* config : a layer configuration ( LayerConfig ) with the arguments used in the current layer instance constructor.","title":"Layer properties and methods"},{"location":"start/tutorial/#using-existing-layers","text":"TensorX ships with a number of built in Layers that you can easily use to compose layer graphs that perform various computations. All layers are accessible from the global namespace tensorx.Linear or from the tensorx.layers module. The following example shows how to use a simple Linear layer that performs the computation y=Wx+b y=Wx+b : import tensorflow as tf import tensorx as tx x = tf . random . uniform ([ 2 , 2 ], dtype = tf . float32 ) # y = Wx + b y = tx . Linear ( x , n_units = 3 ) result = y () assert tx . tensor_equal ( tf . shape ( result ), [ 2 , 3 ]) assert len ( y . inputs ) == 1 assert isinstance ( y . input , tx . Constant ) Note that we can pass a Tensor object to Linear (or any other layer), and it will be automatically converted to a Layer , to a Constant layer to be more precise. The layer y has exactly 1 input layer and __call__ will return the result of its computation on this input.","title":"Using existing Layers"},{"location":"start/tutorial/#dynamic-stateful-input","text":"The Input layer allows us to add a dynamic input to a layer graph: value = tf . random . uniform ([ 2 , 2 ], dtype = tf . float32 ) x = tx . Input ( init_value = value ) # y = Wx + b y = tx . Linear ( x , n_units = 3 ) result1 = y () # x is stateful and its value can be changed e.g. to a new random value x . value = tf . random . uniform ([ 2 , 2 ], dtype = tf . float32 ) result2 = y () result3 = y . compute ( value ) assert not tx . tensor_equal ( result1 , result2 ) assert not y . input . constant # compute returns the layer computation independently from its current graph assert tx . tensor_equal ( result1 , result3 ) print ( result1 ) tf.Tensor ( [[ 0 .8232075 0 .2716378 -0.33215973 ] [ 0 .34996247 -0.02594224 -0.05033442 ]] , shape =( 2 , 3 ) , dtype = float32 ) Input allows the creation of dynamic input layers with a value property that can be changed, we can see that the value at the end-point of this graph changes as well. Moreover, the compute method is distinct from __call__ as it only depends on the layer current state and not on the current graph. Important if n_units is not set to None on a dynamic Input layer, it will take the last dimension of the initial value, henceforth, any tensor assigned to value must match the n_units in its last dimension. This means that the batch dimension can be variable for example. Warning You can't switch the number of dimension in a dynamic Input . Without an initial value or shape, it defaults to a shape (0, 0) (an empty tensor with 2 dimensions). An error is thrown if you try to assign a tensor with a mismatching number of dimensions. For example, if you create an input as follows Input(shape=[None,None,None]) , an error is thrown if you assign a tensor with a mismatching number of dimensions like input.value = tf.ones([2,2]) .","title":"Dynamic stateful Input"},{"location":"start/tutorial/#re-using-layers","text":"When you create a new Layer object, usually you will pass it its input layers which will then make it the end-node of a graph connected to those input layers. This will also call the init_state method which initializes any tf.Variable objects that might be part of the layers' state. If you want to re-use this layer with a different set of input layers, you can use the reuse_with method. This creates a new layer with all the same parameters, additionally this new layer will share it's state with the previous one. import tensorflow as tf import tensorx as tx # stateful input placeholder x1 = tx . Input ( n_units = 2 ) x1 . value = tf . random . uniform ([ 2 , 2 ]) #y = Wx + b l1 = tx . Linear ( x1 , n_units = 3 ) a1 = tx . Activation ( l1 , tx . relu ) l2 = tx . Linear ( a1 , n_units = 4 ) d1 = tx . Dropout ( a1 , probability = 0.4 ) l3 = l2 . reuse_with ( d1 ) Warning Any changes to the state of one layer will affect the state of the second.","title":"Re-Using Layers"},{"location":"start/tutorial/#re-using-modules","text":"A Module is a special layer which creates a single Layer from a given layer graph . A layer graph is a set of layers connected to each other. For example: x = tx . Input ( tf . ones ([ 2 , 2 ])) y1 = tx . Linear ( x , 3 ) y2 = tx . Linear ( y1 , 4 ) m = tx . Module ( inputs = x , output = y2 ) assert tx . tensor_equal ( m (), y2 ()) You can take the two Linear layers and create a single module with a state shared with both layers. Like with any other layer you can also call reuse_with on a module and in this case, the entire state of the two Linear layers will again be shared with the newly created Module .","title":"Re-Using Modules"},{"location":"start/tutorial/#gradients-and-autodiff","text":"Automatic differentiation is a cornerstone of most deep learning frameworks. TensorFlow remembers what operations happen and in what order during the forward pass, then, during the backpropagation pass, TensorFlow traverses this list of operations in reverse order to compute gradients --usually with respect to some input like a tf.Variable . Automatic differentiation can be accessed in Tensorflow using the tf.GradientTape context. Whatever is executed inside the GradientTape context, gets tracked so that the gradients with respect to some variables can be computed: import tensorflow as tf x = tf . Variable ( 3.0 ) with tf . GradientTape () as tape : y = x ** 2 # dy = 2x * dx dy_dx = tape . gradient ( y , x ) TensorX Layers describe operations over tensors in terms of tensorflow operations, and store their state in tf.Variable objects, so layers executed inside the tf.GradientTape context are tracked just like any other Tensorflow operation. With this in mind, we can then compute the gradients of a particular value with respect to the trainable_variables used in the computation. For example: import tensorflow as tf import tensorx as tx x = tx . Input ( n_units = 3 ) # y = Wx + b y = tx . Linear ( x , 3 , add_bias = True ) loss = tx . Lambda ( y , fn = lambda v : tf . reduce_mean ( v ** 2 )) x . value = [[ 1. , 2. , 3. ]] with tf . GradientTape () as tape : loss_value = loss () # we could have done this as well # v = y() # loss_value = tf.reduce_mean(v ** 2) grads = tape . gradient ( loss_value , y . trainable_variables ) assert len ( y . trainable_variables ) == 2 assert len ( grads ) == 2 assert grads [ 0 ] . shape == y . weights . shape assert grads [ 1 ] . shape == y . bias . shape In this case, only the weights , and bias of the Linear layer are trainable variables, so we can take the gradient of loss_value with respect to these variables, the result is a list of tensors with the same shape as the variables used as targets. Tip In these examples we're still using an eager execution model from Tensorflow, as we will see, this is good for debugging, but not very efficient. Next in this tutorial, we will show how we can compile TensorX layer graphs into Tensorflow graphs using the tf.function .","title":"Gradients and Autodiff"},{"location":"start/tutorial/#graph-compilation","text":"TensorX builds layer graphs automatically from layer objects are connected to each other. These graphs are, in effect, directed acyclic graphs (DAG) defining a given computation over inputs. To aid with validation and execution of neural network layer graphs, TensorX has a Graph utility class. The Graph class allows for automatic graph construction from output nodes (by recursively visiting each node's inputs). It also facilitates transversal by dependency ordering along with conversion of arbitrary graphs to functions and Tensorflow static graphs. TensorX takes advantage of Tensorflow's graph optimization system to simplify and optimize Layer computations. It does this by converting layer graphs into functions that are then trace-compiled into an optimized TensorFlow static graphs. x1 = Input ( n_units = 2 ) x2 = Input ( n_units = 4 ) l1 = Linear ( x1 , 4 ) l2 = Add ( l1 , x2 ) l3 = Linear ( l2 , 2 ) g = Graph . build ( outputs = l3 , inputs = [ x1 , x2 ]) fn = g . as_function ( compile = True ) # fn is holding the following function @tf . function def compiled_graph (): x1 = layers [ \"x1\" ] . compute () x2 = layers [ \"x2\" ] . compute () l1 = layers [ \"l1\" ] . compute ( x1 ) l2 = layers [ \"l2\" ] . compute ( l1 , x2 ) l3 = layers [ \"l3\" ] . compute ( l2 ) return l3 If no ord_inputs are given to as_function , the resulting function doesn't define input parameters. To feed values to such a function we would need to change the values of the inputs with x1.value = ... before calling fn() . If ord_inputs are passed (e.g. g.as_function(ord_inputs=[x1,x2]) ), these will map the parameters to the corresponding layers that must be inputs of the current graph, if so, the resulting function can be called with arguments as fn(value1,value2) . Just as Layer objects define implicit subgraphs, we can also build Callable functions and TensorFlow static graphs from any layer by calling layer.as_function() . Much like in the previous example, doing this will return a function without parameters. This is just syntax sugar for: ... graph = Graph . build ( inputs = None , outputs = self ) return graph . as_function ( name = name , compile = compile ) Dev Notes A function conversion procedure which uses parameters with optional values for Input layers is in development.","title":"Graph Compilation"},{"location":"start/tutorial/#models","text":"TensorX uses the Model class to group together multiple layer graphs and simplify the configuration of a training loop with multiple callbacks. This part of the API might suffer some changes but at its core it's just intended as a way to group together layer graphs, optimizers , and a configurable training loop with Callbacks . Docs in progress Finish this documentation with examples","title":"Models"},{"location":"start/tutorial/#callbacks","text":"Docs in progress Finish this documentation with examples","title":"Callbacks"},{"location":"start/tutorial/#serialization","text":"Docs in progress Finish this documentation with examples","title":"Serialization"}]}